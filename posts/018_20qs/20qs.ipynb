{
 "cells": [
  {
   "cell_type": "raw",
   "id": "083334a8-dee9-4fcd-a0a9-8621819cc222",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "\n",
    "  output: asis\n",
    "\n",
    "\n",
    "title: \"Let's play 20 questions with o3!\"\n",
    "description: \"I'm thinking of a character...\"\n",
    "author: \"Julia Levine\"\n",
    "date: \"5/13/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Games\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2d831-3d24-423b-9c36-ed6bb4b1aafc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "First it was the 20 questions ball, then it was the Akinator, now its o1! I am super curious to how o3's \"advanced reasoning\" goes up against my stupid topics and words in a classic game of 20 questions! I predict that o3 may miss the point on a few rounds, whereas the 20 questions ball classically has an insane logic flow that determines most topics with (eventual) ease. Because o3 has the \"free will\" of determining its own logic, I am not so sure it will actually guess my topic.\n",
    "\n",
    "For this particular experiment, I will start with a 4o round of 20 questions to see if there is any difference in reasoning.\n",
    "Just so you know I'm not tricking gpt-4o, my topic for this round is *mirror*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991deb5-ed8d-4bcc-bb27-08557a1271ba",
   "metadata": {},
   "source": [
    "# Round 1 Discussion\n",
    "I would compare 4o's question capability to the same level as the 20 questions ball or akinator. Questions had a flow and logic to them and were descriptive based on cues given from previous questions. However, 4o ended up guessing a razor instead of a mirror (unfortunate!). \n",
    "\n",
    "Let's try round 2 with o3. My topic for this round is *sandwich*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4007b-19f9-4de7-a3fb-34021d3a75e7",
   "metadata": {},
   "source": [
    "# Round 2 Discussion\n",
    "o3 is horrible at 20 questions. I honestly think the 20 questions ball from like the 80s would have been able to do better which is... interesting. Throughout the round o3 asked me constantly a basic yes or no question on if my object was material X. This used up about 15 of the 20 questions, as o3 cycled through every possible material (all construction-based even though my object was food-based) and then gave up and finally decided to ask if my object was food-related on the *20th* guess. It then proceeded to guess pizza, which was close and relatively impressive for a 1-question guess, but still wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d195a5b-f95b-4b6b-a8bc-8e57001c88d8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I'm not sure if 4o was actually better than o3, but the level of close-ness while still missing the mark that 4o achieved seemed to be more on par with the Akinator competition. For example, 4o had a clear variation of questions asked, whereas o3 got stuck along one topic that wasn't even relevant to my object and wasted all the questions because of its logical fault. I will note that in o3's thinking highlights, there was a lot of mention of \"I must figure out the material to determine what kind of object it is,\" which is a limited and in this case critical error. \n",
    "\n",
    "While the 20 questions ball and the Akinator have highly developed logic in order to make predictions, I had thought that ChatGPT could perform on-par with those competitors due to its free reasoning capability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
