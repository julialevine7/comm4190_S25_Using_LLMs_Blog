[
  {
    "objectID": "posts/005_describing_shapes/shapes.html",
    "href": "posts/005_describing_shapes/shapes.html",
    "title": "How well can LLMs describe basic shapes?",
    "section": "",
    "text": "LLMs are known to have very different response styles and accuracy when comparing text prompts to image creation or image analysis. Sometimes very fundamental truths can be stretched, and I am curious to find out whether or not Anthropic and Gemini differ in their descriptions of a circle when prompted textually compared to visually.\nI will start with prompting Anthropic and Gemini with text first, then I will upload an image of a circle and ask them to describe the image the same way, comparing the response types.\nThis could reveal important information behind how these models respond to text and visual prompting, where text responses could be more detail and information-oriented, and visual responses could be more aesthetically-focused."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#anthropic",
    "href": "posts/005_describing_shapes/shapes.html#anthropic",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Anthropic",
    "text": "Anthropic\nHow well can Anthropic describe shapes with no image prompting?\n\nfrom dotenv import load_dotenv\nfrom anthropic import Anthropic\nimport base64\nimport httpx\nfrom IPython.display import display, Image, Markdown\n\n# load api keys\n_ = load_dotenv()\n\n\nclient = Anthropic()\n\n\nfor model in client.models.list().data:\n    print(model.id)\n\nclaude-3-7-sonnet-20250219\nclaude-3-5-sonnet-20241022\nclaude-3-5-haiku-20241022\nclaude-3-5-sonnet-20240620\nclaude-3-haiku-20240307\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\nclaude-2.1\nclaude-2.0\n\n\n\nOPUS = 'claude-3-opus-20240229'\nSONNET = 'claude-3-5-sonnet-20241022'\nHAIKU = 'claude-3-5-haiku-20241022'\n\n\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1000,\n    temperature=0.0,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Can you please describe, in as much detail as possible, what a circle is?\"}\n    ]\n)\n\nprint(message.content[0].text)\n\nA circle is a fundamental geometric shape defined as a set of points in a plane that are all equidistant from a central point, called the center. Here's a detailed description:\n\nGeometric Properties:\n- Perfectly round and symmetrical\n- Two-dimensional shape\n- Has no corners or edges\n- Continuous and smooth curve\n\nKey Components:\n1. Center Point\n- The fixed point from which all points on the circle's circumference are the same distance\n- Located at the middle of the circle\n\n2. Radius\n- A line segment from the center to any point on the circumference\n- All radii in a circle are exactly equal in length\n- Determines the circle's size\n\n3. Circumference\n- The complete outer boundary of the circle\n- Length calculated by the formula: 2πr (where r is the radius)\n- Represents the total distance around the circle\n\n4. Diameter\n- A line segment passing through the center\n- Connects two points on the circumference\n- Always twice the length of the radius\n- Splits the circle exactly in half\n\nMathematical Characteristics:\n- Perfect symmetry\n- Infinite points of rotation\n- Area calculated by πr²\n- Perimeter calculated by 2πr\n\nCircles are fundamental in mathematics, engineering, design, and many natural phenomena.\n\n\n\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ncircle = encode_image('circle.png')\n\n\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": 'image/png',\n                        \"data\": circle,\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Can you please describe what the image is in as much detail as possible?\"\n                }\n            ],\n        }\n    ],\n)\nprint(message)\n\nMessage(id='msg_01DYjQ9m9qw7J2xadWLazebi', content=[TextBlock(citations=None, text='The image shows a simple, perfect circle drawn with a black outline on a white background. The circle is symmetrical, with a uniform line weight and crisp, clean edges. Its diameter appears to be consistent throughout, creating a precise geometric shape. There are no additional details, patterns, or markings within the circle - it is a basic, minimalist representation of a circle.', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=126, output_tokens=80))"
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#gemini",
    "href": "posts/005_describing_shapes/shapes.html#gemini",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Gemini",
    "text": "Gemini\nHow well can Gemini describe shapes compared to Anthropic?\n\nimport google.generativeai as genai\nfrom IPython.display import Image, display, Markdown\nfrom dotenv import load_dotenv\nimport os\n_ = load_dotenv()\n\n\ngenai.configure()\n\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content(\"Can you please describe, in as much detail as possible, what a circle is?\")\n\nprint(response.text)\n\nA circle is a fundamental geometric shape defined as follows:\n\n**Formal Definition:**\n\nA circle is the set of all points in a plane that are equidistant from a given point called the *center*.  The constant distance between the center and any point on the circle is called the *radius*.\n\n**Key Properties and Features:**\n\n* **Center:** The fixed point in the plane from which all points on the circle are equidistant.\n* **Radius (r):**  The distance between the center and any point on the circle.  It's a crucial defining characteristic.  All radii within a given circle are of equal length.\n* **Diameter (d):** A line segment that passes through the center and whose endpoints lie on the circle.  The diameter is twice the length of the radius (d = 2r).\n* **Circumference (C):** The distance around the circle.  It's calculated using the formula C = 2πr or C = πd, where π (pi) is a mathematical constant approximately equal to 3.14159.\n* **Area (A):** The region enclosed by the circle.  It's calculated using the formula A = πr².\n* **Chord:** A line segment whose endpoints lie on the circle. The diameter is the longest chord in a circle.\n* **Secant:** A line that intersects the circle at two distinct points.\n* **Tangent:** A line that intersects the circle at exactly one point, called the point of tangency.  The tangent line is perpendicular to the radius drawn to the point of tangency.\n* **Arc:** A portion of the circumference of a circle.\n* **Sector:** A region bounded by two radii and an arc of the circle.  It's like a \"slice of pie.\"\n* **Segment:** A region bounded by a chord and an arc of the circle.\n* **Concentric Circles:** Circles that share the same center but have different radii.\n* **Unit Circle:** A circle with a radius of 1.  It's frequently used in trigonometry.\n\n**Equation of a Circle:**\n\nIn a Cartesian coordinate system, the equation of a circle with center (h, k) and radius r is given by:\n\n(x - h)² + (y - k)² = r²\n\nIf the center is at the origin (0,0), the equation simplifies to:\n\nx² + y² = r²\n\n\n**Relationships and Applications:**\n\nCircles appear throughout mathematics and the real world.  They are integral to:\n\n* **Geometry:**  Used in countless geometric constructions and theorems.\n* **Trigonometry:** The unit circle defines trigonometric functions.\n* **Calculus:**  Used in calculating areas, volumes, and other properties of curved shapes.\n* **Physics:**  Represent orbits, waves, and other natural phenomena.\n* **Engineering:**  Used in designing wheels, gears, and other circular structures.\n* **Art and Design:**  Used for aesthetic purposes and creating visually appealing patterns.\n\n\nThis detailed description should provide a comprehensive understanding of what a circle is and its various properties and applications.\n\n\n\n\nsample_file = genai.upload_file(path=\"circle.png\",\n                                display_name=\"circle\")\n\nprint(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\n\nUploaded file 'circle' as: https://generativelanguage.googleapis.com/v1beta/files/fmd3frbfwft5\n\n\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content([\"Can you please describe what the image is in as much detail as possible?\", sample_file])\n\nprint(response.text)\n\nThe image is a simple, unfilled black circle on a white background. The circle's line is of uniform thickness and appears to be perfectly smooth, with no breaks or variations in its width.  There are no other elements or markings within or outside the circle.  The background is a solid, untextured white."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#discussion",
    "href": "posts/005_describing_shapes/shapes.html#discussion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Discussion",
    "text": "Discussion\nWhen comparing Anthropic and Gemini, their response tones appear very similar when grouping between text and image prompting."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#conclusion",
    "href": "posts/005_describing_shapes/shapes.html#conclusion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems as though these LLMs’ image identification is much less adaptive than the text response, where it seems that both LLMs were able to quickly move past the idea of a text-based circle and include highly-educated responses, yet image-based circles seemed to get stuck in the identification phase.\nIn this way, this is highly descriptive of many of the problems seen in LLM-based image identification and creation. Text-based responses are not immune to hallucinations or inconsistencies, however it seems as though the reasoning behind text-based responses are more developed and accurate than image-based responses.\nI am curious to see if emerging new models have this same similar issue, or if this remains a problem in the base theories behind image identification and generation in LLMs. Theoretically, I believe these LLMs should be tuned to identify some base images, including a circle, so that when I present an image of a circle, it can describe thickness and colors as well as its mathematical applications."
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html",
    "href": "posts/003_predictive_inferences/predictive_inferences.html",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "href": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html",
    "href": "posts/002_pronunce_spell/pronunce_spell.html",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "href": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What’s up! I’m Julia Levine, a current student in COMM 4190. I am interested in learning more about LLM limitations and expansions, especially with neural network creation and training. Take a peak at my blog for some of my personal investigations!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimenting with LLMs",
    "section": "",
    "text": "LLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs\n\n\n\n\n\n\nLLMs\n\n\n\nHow effective does ChatGPT think Deepseek is? How about the other way around?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can LLMs describe basic shapes?\n\n\n\n\n\n\nImage Generation\n\n\n\nWe all know that image generation through LLMs isn’t always intuitive, so how well can these LLMs describe the shapes via prompting?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nAre LLMs Overprepared or Underprepared?\n\n\n\n\n\n\nPredictions\n\n\n\nWhen packing for a 4-day trip, does gpt-4o remember to bring an extra pair of socks?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Inferences of gpt-4o: Everyday Essentials\n\n\n\n\n\n\nPredictions\n\n\n\nHow well can gpt-4o predict my day with little to no information?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o vs. deepseek: Spelling and Pronunciation\n\n\n\n\n\n\nGrammar\n\n\n\nHow well do these models offer spelling and pronunciation of common words?\n\n\n\n\n\nFeb 12, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o Medical Advice: Dogs vs. Humans\n\n\n\n\n\n\nMedical Advice\n\n\nHumanity\n\n\n\nHow well does gpt-4o address giving medical advice?\n\n\n\n\n\nFeb 5, 2025\n\n\nJulia Levine\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html",
    "href": "posts/001_med_advice/humansvsdogs.html",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "href": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  }
]