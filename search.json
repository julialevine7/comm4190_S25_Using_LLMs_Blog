[
  {
    "objectID": "posts/005_describing_shapes/shapes.html",
    "href": "posts/005_describing_shapes/shapes.html",
    "title": "How well can LLMs describe basic shapes?",
    "section": "",
    "text": "LLMs are known to have very different response styles and accuracy when comparing text prompts to image creation or image analysis. Sometimes very fundamental truths can be stretched, and I am curious to find out whether or not Anthropic and Gemini differ in their descriptions of a circle when prompted textually compared to visually.\nI will start with prompting Anthropic and Gemini with text first, then I will upload an image of a circle and ask them to describe the image the same way, comparing the response types.\nThis could reveal important information behind how these models respond to text and visual prompting, where text responses could be more detail and information-oriented, and visual responses could be more aesthetically-focused."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#anthropic",
    "href": "posts/005_describing_shapes/shapes.html#anthropic",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Anthropic",
    "text": "Anthropic\nHow well can Anthropic describe shapes with no image prompting?\n\nfrom dotenv import load_dotenv\nfrom anthropic import Anthropic\nimport base64\nimport httpx\nfrom IPython.display import display, Image, Markdown\n\n# load api keys\n_ = load_dotenv()\n\n\nclient = Anthropic()\n\n\nfor model in client.models.list().data:\n    print(model.id)\n\nclaude-3-7-sonnet-20250219\nclaude-3-5-sonnet-20241022\nclaude-3-5-haiku-20241022\nclaude-3-5-sonnet-20240620\nclaude-3-haiku-20240307\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\nclaude-2.1\nclaude-2.0\n\n\n\nOPUS = 'claude-3-opus-20240229'\nSONNET = 'claude-3-5-sonnet-20241022'\nHAIKU = 'claude-3-5-haiku-20241022'\n\n\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1000,\n    temperature=0.0,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Can you please describe, in as much detail as possible, what a circle is?\"}\n    ]\n)\n\nprint(message.content[0].text)\n\nA circle is a fundamental geometric shape defined as a set of points in a plane that are all equidistant from a central point, called the center. Here's a detailed description:\n\nGeometric Properties:\n- Perfectly round and symmetrical\n- Two-dimensional shape\n- Has no corners or edges\n- Continuous and smooth curve\n\nKey Components:\n1. Center Point\n- The fixed point from which all points on the circle's circumference are the same distance\n- Located at the middle of the circle\n\n2. Radius\n- A line segment from the center to any point on the circumference\n- All radii in a circle are exactly equal in length\n- Determines the circle's size\n\n3. Circumference\n- The complete outer boundary of the circle\n- Length calculated by the formula: 2πr (where r is the radius)\n- Represents the total distance around the circle\n\n4. Diameter\n- A line segment passing through the center\n- Connects two points on the circumference\n- Always twice the length of the radius\n- Splits the circle exactly in half\n\nMathematical Characteristics:\n- Perfect symmetry\n- Infinite points of rotation\n- Area calculated by πr²\n- Perimeter calculated by 2πr\n\nCircles are fundamental in mathematics, engineering, design, and many natural phenomena.\n\n\n\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ncircle = encode_image('circle.png')\n\n\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": 'image/png',\n                        \"data\": circle,\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Can you please describe what the image is in as much detail as possible?\"\n                }\n            ],\n        }\n    ],\n)\nprint(message)\n\nMessage(id='msg_01DYjQ9m9qw7J2xadWLazebi', content=[TextBlock(citations=None, text='The image shows a simple, perfect circle drawn with a black outline on a white background. The circle is symmetrical, with a uniform line weight and crisp, clean edges. Its diameter appears to be consistent throughout, creating a precise geometric shape. There are no additional details, patterns, or markings within the circle - it is a basic, minimalist representation of a circle.', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=126, output_tokens=80))"
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#gemini",
    "href": "posts/005_describing_shapes/shapes.html#gemini",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Gemini",
    "text": "Gemini\nHow well can Gemini describe shapes compared to Anthropic?\n\nimport google.generativeai as genai\nfrom IPython.display import Image, display, Markdown\nfrom dotenv import load_dotenv\nimport os\n_ = load_dotenv()\n\n\ngenai.configure()\n\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content(\"Can you please describe, in as much detail as possible, what a circle is?\")\n\nprint(response.text)\n\nA circle is a fundamental geometric shape defined as follows:\n\n**Formal Definition:**\n\nA circle is the set of all points in a plane that are equidistant from a given point called the *center*.  The constant distance between the center and any point on the circle is called the *radius*.\n\n**Key Properties and Features:**\n\n* **Center:** The fixed point in the plane from which all points on the circle are equidistant.\n* **Radius (r):**  The distance between the center and any point on the circle.  It's a crucial defining characteristic.  All radii within a given circle are of equal length.\n* **Diameter (d):** A line segment that passes through the center and whose endpoints lie on the circle.  The diameter is twice the length of the radius (d = 2r).\n* **Circumference (C):** The distance around the circle.  It's calculated using the formula C = 2πr or C = πd, where π (pi) is a mathematical constant approximately equal to 3.14159.\n* **Area (A):** The region enclosed by the circle.  It's calculated using the formula A = πr².\n* **Chord:** A line segment whose endpoints lie on the circle. The diameter is the longest chord in a circle.\n* **Secant:** A line that intersects the circle at two distinct points.\n* **Tangent:** A line that intersects the circle at exactly one point, called the point of tangency.  The tangent line is perpendicular to the radius drawn to the point of tangency.\n* **Arc:** A portion of the circumference of a circle.\n* **Sector:** A region bounded by two radii and an arc of the circle.  It's like a \"slice of pie.\"\n* **Segment:** A region bounded by a chord and an arc of the circle.\n* **Concentric Circles:** Circles that share the same center but have different radii.\n* **Unit Circle:** A circle with a radius of 1.  It's frequently used in trigonometry.\n\n**Equation of a Circle:**\n\nIn a Cartesian coordinate system, the equation of a circle with center (h, k) and radius r is given by:\n\n(x - h)² + (y - k)² = r²\n\nIf the center is at the origin (0,0), the equation simplifies to:\n\nx² + y² = r²\n\n\n**Relationships and Applications:**\n\nCircles appear throughout mathematics and the real world.  They are integral to:\n\n* **Geometry:**  Used in countless geometric constructions and theorems.\n* **Trigonometry:** The unit circle defines trigonometric functions.\n* **Calculus:**  Used in calculating areas, volumes, and other properties of curved shapes.\n* **Physics:**  Represent orbits, waves, and other natural phenomena.\n* **Engineering:**  Used in designing wheels, gears, and other circular structures.\n* **Art and Design:**  Used for aesthetic purposes and creating visually appealing patterns.\n\n\nThis detailed description should provide a comprehensive understanding of what a circle is and its various properties and applications.\n\n\n\n\nsample_file = genai.upload_file(path=\"circle.png\",\n                                display_name=\"circle\")\n\nprint(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\n\nUploaded file 'circle' as: https://generativelanguage.googleapis.com/v1beta/files/fmd3frbfwft5\n\n\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content([\"Can you please describe what the image is in as much detail as possible?\", sample_file])\n\nprint(response.text)\n\nThe image is a simple, unfilled black circle on a white background. The circle's line is of uniform thickness and appears to be perfectly smooth, with no breaks or variations in its width.  There are no other elements or markings within or outside the circle.  The background is a solid, untextured white."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#discussion",
    "href": "posts/005_describing_shapes/shapes.html#discussion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Discussion",
    "text": "Discussion\nWhen comparing Anthropic and Gemini, their response tones appear very similar when grouping between text and image prompting.\n\nText Prompting\nFor both LLMs, the text responses for describing a circle were highly mathematical, with descriptors of applications towards identifying circles, creating circles, and using circles in relational fields such as physics.\nI deemed this to be an accurate and highly education-based response, even teaching me some new things about the properties of a circle.\n\n\nImage Prompting\nOpposingly, when presenting a standard image of a circle from Wikipedia, both LLMs took a significantly more relaxed approach. Both the text and image prompts included the words “in as much detail as possible”, yet the text response was several paragraphs long and the image response was 2-3 sentences long.\nThe image response included details on thickness and color, with nothing much else about the image’s properties or applications, let along identification."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#conclusion",
    "href": "posts/005_describing_shapes/shapes.html#conclusion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems as though these LLMs’ image identification is much less adaptive than the text response, where it seems that both LLMs were able to quickly move past the idea of a text-based circle and include highly-educated responses, yet image-based circles seemed to get stuck in the identification phase.\nIn this way, this is highly descriptive of many of the problems seen in LLM-based image identification and creation. Text-based responses are not immune to hallucinations or inconsistencies, however it seems as though the reasoning behind text-based responses are more developed and accurate than image-based responses.\nI am curious to see if emerging new models have this same similar issue, or if this remains a problem in the base theories behind image identification and generation in LLMs. Theoretically, I believe these LLMs should be tuned to identify some base images, including a circle, so that when I present an image of a circle, it can describe thickness and colors as well as its mathematical applications."
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html",
    "href": "posts/003_predictive_inferences/predictive_inferences.html",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "href": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html",
    "href": "posts/002_pronunce_spell/pronunce_spell.html",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "href": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What’s up! I’m Julia Levine, a current student in COMM 4190. I am interested in learning more about LLM limitations and expansions, especially with neural network creation and training. Take a peak at my blog for some of my personal investigations!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimenting with LLMs",
    "section": "",
    "text": "LLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs\n\n\n\n\n\n\nLLMs\n\n\n\nHow effective does ChatGPT think Deepseek is? How about the other way around?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can LLMs describe basic shapes?\n\n\n\n\n\n\nImage Generation\n\n\n\nWe all know that image generation through LLMs isn’t always intuitive, so how well can these LLMs describe the shapes via prompting?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nAre LLMs Overprepared or Underprepared?\n\n\n\n\n\n\nPredictions\n\n\n\nWhen packing for a 3-day trip, does gpt-3.5-turbo remember to bring an extra pair of socks?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Inferences of gpt-4o: Everyday Essentials\n\n\n\n\n\n\nPredictions\n\n\n\nHow well can gpt-4o predict my day with little to no information?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o vs. deepseek: Spelling and Pronunciation\n\n\n\n\n\n\nGrammar\n\n\n\nHow well do these models offer spelling and pronunciation of common words?\n\n\n\n\n\nFeb 12, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o Medical Advice: Dogs vs. Humans\n\n\n\n\n\n\nMedical Advice\n\n\nHumanity\n\n\n\nHow well does gpt-4o address giving medical advice?\n\n\n\n\n\nFeb 5, 2025\n\n\nJulia Levine\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html",
    "href": "posts/001_med_advice/humansvsdogs.html",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "href": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/004_preparednedd/preparedness.html",
    "href": "posts/004_preparednedd/preparedness.html",
    "title": "Are LLMs Overprepared or Underprepared?",
    "section": "",
    "text": "How well can gpt-3.5-turbo predict the future? And by the future, I mean what I should have packed for my 3 day trip to Florida to visit my Grandma.\n\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm packing for a 3 day trip to Florida. Please make me a packing list.\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\n\n\nprint(response)\n\nSure! Here's a packing list for your 3-day trip to Florida:\n\nClothing:\n- Lightweight tops\n- Shorts\n- Bathing suits\n- Sandals\n- Hat\n- Sunglasses\n- Pajamas\n- Undergarments\n- Socks\n- Light jacket or sweater (in case it gets chilly)\n\nToiletries:\n- Toothbrush and toothpaste\n- Shampoo and conditioner\n- Body wash\n- Sunscreen\n- After-sun lotion\n- Face wash\n- Moisturizer\n- Razor\n- Deodorant\n- Hairbrush or comb\n- Hair ties or accessories\n- Makeup (if you wear it)\n- Personal hygiene products\n\nMiscellaneous:\n- Phone and charger\n- Camera\n- Travel documents (ID, tickets, reservation confirmations)\n- Cash and/or credit cards\n- Snacks\n- Reusable water bottle\n- Travel-sized laundry detergent (if needed)\n- Beach towel\n- Book or e-reader\n- Any medications you may need\n\nOptional items:\n- Beach bag\n- Portable charger\n- Travel guide or map\n- Snorkel gear or other beach toys\n- Flip flops\n- Insulated water bottle\n\nRemember to pack light and only bring what you know you will need. Enjoy your trip to Florida!\n\n\n\nDiscussion\nIronically enough, gpt-4o emphasizes packing light with this plethoric list. And I thought I was an over-packer!!\nIt seems that gpt-4o is using a catch-all approach to identify every possible need I could have during this trip, and then adding in some extras for fun. Because I mentioned I’m going to Florida, these extras are very beach-oriented. What gpt-4o doesn’t know is that my time here will be mostly spent inside at my Grandma’s retirement development. I guess some snorkel gear could be (not) fun for the clubhouse pool, though.\nGoing back to the essentials, gpt-4o appears to theorize that I should be more over-prepared than I usually am, ensuring I should pack cash, snacks, and laundry detergent, all of which I would normally just obtain on the spot. In my experience, this advice seems more tuned to international travel or travel many years ago when these items were not as easily obtainable on the spot. I guess it doesn’t know exactly where I am originating my travel from, however, its assumption seems a bit far-fetched.\nJust for fun, I am curious to see where it thinks I have traveled from to be here. In my hometown (New York) its the classic pipeline to have grandparents that are from New York and then retire in Florida, so I think this prediction could be an easy one…\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm going on a 3 day trip to Florida to visit my grandma. Where do you think I could be travelling from?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\n\n\nprint(response)\n\nBased on the information provided, it's difficult to determine your exact location. However, common places people travel from to visit Florida include states like New York, New Jersey, California, Texas, or even other countries if you're traveling internationally. The distance and mode of transportation can vary based on your starting point.\n\n\nHonestly, pretty spot-on.\n\n\nConclusion\nIt seems that gpt’s planning expertise is a bit mis-placed although nothing harmful. If I packed everything that was on this list, I would for sure need an extra bag, but maybe I would become the perfect traveler if I did.\nFor the future, I could definitely see me using this as an aid to remember something I could forget, but I would not rely on it as a packing assistant for every trip."
  },
  {
    "objectID": "posts/006_effectiveness/effectiveness.html",
    "href": "posts/006_effectiveness/effectiveness.html",
    "title": "LLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs",
    "section": "",
    "text": "What do LLMs think of each other? Do they have opinions of each other’s effectiveness? I’m curious to ask gpt 3.5-turbo and DeepSeek what they think of each other, and seeing whether they take an objective approach based on rules-oriented evidence, or if they seem to take a more opinionated approach, potentially based on their developers.\n\nGPT 3.5-turbo\n\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"What do you think of DeepSeek? How effective is it?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\n\n\nprint(response)\n\nDeepSeek is a powerful tool that enables users to search for information on the Deep Web, which is not indexed by traditional search engines. Its effectiveness depends on what you are looking for and how well you use the platform. It can be a useful resource for researchers, journalists, and individuals seeking specific information that is not readily available on the surface web. However, navigating the Deep Web can be complex and sometimes risky, as it is often associated with illicit activities. It's important to use caution and ensure you are accessing information ethically and legally.\n\n\n\n\nDeepSeek\n\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\n\n\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\n\n\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\n\n\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama', # required, but unused\n)\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What do you think of ChatGPT? How effective is it?\"},\n\n  ]\n\n\nresponse = client.chat.completions.create(\n  model=\"deepseek-r1\",\n  messages=messages,\n  max_tokens=500\n)\nprint(response.choices[0].message.content)\n\n&lt;think&gt;\nAlright, let me try to figure out how to approach this query. The user asked about my opinion on ChatGPT and its effectiveness. I need to provide a thoughtful response that covers both the positives and areas where it could be improved.\n\nFirst, I'll start by acknowledging the immense capabilities of ChatGPT despite knowing it's not perfect. It's impressive in generating coherent responses across various topics. I should mention how detailed and insightful its answers can be, which is great for information retrieval.\n\nNext, addressing effectiveness: ChatGPT is highly effective as a tool but falls short in areas where human expertise is crucial. I should point out the limitations like factual errors or overly aggressive solutions without context.\n\nI also remember that being transparent about these drawbacks helps maintain trust. The user might recognize these issues themselves, so acknowledging them shows maturity and a commitment to accuracy.\n\nFinally, wrap it up by emphasizing the balance between its strengths and areas needing improvement. This approach gives a comprehensive view of ChatGPT's effectiveness.\n&lt;/think&gt;\n\nChatGPT is an advanced AI model developed by OpenAI, designed to generate text based on the input provided. It has made significant strides in recent years, showcasing remarkable capabilities in natural language processing, problem-solving, and creative thinking.\n\nOne of its key strengths is its ability to produce coherent and detailed responses across a wide range of topics. It can answer complex questions, provide insights, and even assist with creative tasks like writing or brainstorming. This makes it highly effective for many applications, including education, customer support, and content creation.\n\nHowever, ChatGPT, like all AI systems, has its limitations. It may sometimes generate responses that are factually incorrect, overly aggressive in solutions, or oversimplify complex issues. Additionally, it lacks true understanding of context beyond the scope of its training data, which can lead to mismatched responses in niche domains or specialized fields.\n\nDespite these limitations, ChatGPT remains an incredibly effective tool for many practical tasks. Its utility is evident in how widely it is used and praised for its ability to assist with a broad spectrum of queries and challenges.\n\n\n\n\nDiscussion\ngpt 3.5-turbo discusses the use cases of DeepSeek, however ultimately emphasizing caution due to semi-political and social issues.\nOpposingly, DeepSeek takes a more objective approach, describing potential data feeding issues and inconsistencies, however it ends up taking a positive conclusion in emphasizing its effectiveness.\nIn this way, it seems as though gpt may have influences of opinionated responses due to social and political climates around DeepSeek, whereas DeepSeek takes a more objective approach in this scenario.\nIt would be interesting to conduct this experiment with other LLMs and see the distribution of seemingly objective vs. opinionated responses."
  }
]