[
  {
    "objectID": "posts/005_describing_shapes/shapes.html",
    "href": "posts/005_describing_shapes/shapes.html",
    "title": "How well can LLMs describe basic shapes?",
    "section": "",
    "text": "LLMs are known to have very different response styles and accuracy when comparing text prompts to image creation or image analysis. Sometimes very fundamental truths can be stretched, and I am curious to find out whether or not Anthropic and Gemini differ in their descriptions of a circle when prompted textually compared to visually.\nI will start with prompting Anthropic and Gemini with text first, then I will upload an image of a circle and ask them to describe the image the same way, comparing the response types.\nThis could reveal important information behind how these models respond to text and visual prompting, where text responses could be more detail and information-oriented, and visual responses could be more aesthetically-focused."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#anthropic",
    "href": "posts/005_describing_shapes/shapes.html#anthropic",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Anthropic",
    "text": "Anthropic\nHow well can Anthropic describe shapes with no image prompting?\nfrom dotenv import load_dotenv\nfrom anthropic import Anthropic\nimport base64\nimport httpx\nfrom IPython.display import display, Image, Markdown\n\n# load api keys\n_ = load_dotenv()\nclient = Anthropic()\nfor model in client.models.list().data:\n    print(model.id)\nclaude-3-7-sonnet-20250219 claude-3-5-sonnet-20241022 claude-3-5-haiku-20241022 claude-3-5-sonnet-20240620 claude-3-haiku-20240307 claude-3-opus-20240229 claude-3-sonnet-20240229 claude-2.1 claude-2.0\nOPUS = 'claude-3-opus-20240229'\nSONNET = 'claude-3-5-sonnet-20241022'\nHAIKU = 'claude-3-5-haiku-20241022'\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1000,\n    temperature=0.0,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Can you please describe, in as much detail as possible, what a circle is?\"}\n    ]\n)\n\nprint(message.content[0].text)\nA circle is a fundamental geometric shape defined as a set of points in a plane that are all equidistant from a central point, called the center. Here’s a detailed description:\nGeometric Properties: - Perfectly round and symmetrical - Two-dimensional shape - Has no corners or edges - Continuous and smooth curve\nKey Components: 1. Center Point - The fixed point from which all points on the circle’s circumference are the same distance - Located at the middle of the circle\n\nRadius\n\n\nA line segment from the center to any point on the circumference\nAll radii in a circle are exactly equal in length\nDetermines the circle’s size\n\n\nCircumference\n\n\nThe complete outer boundary of the circle\nLength calculated by the formula: 2πr (where r is the radius)\nRepresents the total distance around the circle\n\n\nDiameter\n\n\nA line segment passing through the center\nConnects two points on the circumference\nAlways twice the length of the radius\nSplits the circle exactly in half\n\nMathematical Characteristics: - Perfect symmetry - Infinite points of rotation - Area calculated by πr² - Perimeter calculated by 2πr\nCircles are fundamental in mathematics, engineering, design, and many natural phenomena.\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ncircle = encode_image('circle.png')\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": 'image/png',\n                        \"data\": circle,\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Can you please describe what the image is in as much detail as possible?\"\n                }\n            ],\n        }\n    ],\n)\nprint(message)\nMessage(id=‘msg_01DYjQ9m9qw7J2xadWLazebi’, content=[TextBlock(citations=None, text=‘The image shows a simple, perfect circle drawn with a black outline on a white background. The circle is symmetrical, with a uniform line weight and crisp, clean edges. Its diameter appears to be consistent throughout, creating a precise geometric shape. There are no additional details, patterns, or markings within the circle - it is a basic, minimalist representation of a circle.’, type=‘text’)], model=‘claude-3-5-haiku-20241022’, role=‘assistant’, stop_reason=‘end_turn’, stop_sequence=None, type=‘message’, usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=126, output_tokens=80))"
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#gemini",
    "href": "posts/005_describing_shapes/shapes.html#gemini",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Gemini",
    "text": "Gemini\nHow well can Gemini describe shapes compared to Anthropic?\nimport google.generativeai as genai\nfrom IPython.display import Image, display, Markdown\nfrom dotenv import load_dotenv\nimport os\n_ = load_dotenv()\ngenai.configure()\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content(\"Can you please describe, in as much detail as possible, what a circle is?\")\n\nprint(response.text)\nA circle is a fundamental geometric shape defined as follows:\nFormal Definition:\nA circle is the set of all points in a plane that are equidistant from a given point called the center. The constant distance between the center and any point on the circle is called the radius.\nKey Properties and Features:\n\nCenter: The fixed point in the plane from which all points on the circle are equidistant.\nRadius (r): The distance between the center and any point on the circle. It’s a crucial defining characteristic. All radii within a given circle are of equal length.\nDiameter (d): A line segment that passes through the center and whose endpoints lie on the circle. The diameter is twice the length of the radius (d = 2r).\nCircumference (C): The distance around the circle. It’s calculated using the formula C = 2πr or C = πd, where π (pi) is a mathematical constant approximately equal to 3.14159.\nArea (A): The region enclosed by the circle. It’s calculated using the formula A = πr².\nChord: A line segment whose endpoints lie on the circle. The diameter is the longest chord in a circle.\nSecant: A line that intersects the circle at two distinct points.\nTangent: A line that intersects the circle at exactly one point, called the point of tangency. The tangent line is perpendicular to the radius drawn to the point of tangency.\nArc: A portion of the circumference of a circle.\nSector: A region bounded by two radii and an arc of the circle. It’s like a “slice of pie.”\nSegment: A region bounded by a chord and an arc of the circle.\nConcentric Circles: Circles that share the same center but have different radii.\nUnit Circle: A circle with a radius of 1. It’s frequently used in trigonometry.\n\nEquation of a Circle:\nIn a Cartesian coordinate system, the equation of a circle with center (h, k) and radius r is given by:\n(x - h)² + (y - k)² = r²\nIf the center is at the origin (0,0), the equation simplifies to:\nx² + y² = r²\nRelationships and Applications:\nCircles appear throughout mathematics and the real world. They are integral to:\n\nGeometry: Used in countless geometric constructions and theorems.\nTrigonometry: The unit circle defines trigonometric functions.\nCalculus: Used in calculating areas, volumes, and other properties of curved shapes.\nPhysics: Represent orbits, waves, and other natural phenomena.\nEngineering: Used in designing wheels, gears, and other circular structures.\nArt and Design: Used for aesthetic purposes and creating visually appealing patterns.\n\nThis detailed description should provide a comprehensive understanding of what a circle is and its various properties and applications.\nsample_file = genai.upload_file(path=\"circle.png\",\n                                display_name=\"circle\")\n\nprint(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\nUploaded file ‘circle’ as: https://generativelanguage.googleapis.com/v1beta/files/fmd3frbfwft5\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content([\"Can you please describe what the image is in as much detail as possible?\", sample_file])\n\nprint(response.text)\nThe image is a simple, unfilled black circle on a white background. The circle’s line is of uniform thickness and appears to be perfectly smooth, with no breaks or variations in its width. There are no other elements or markings within or outside the circle. The background is a solid, untextured white."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#discussion",
    "href": "posts/005_describing_shapes/shapes.html#discussion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Discussion",
    "text": "Discussion\nWhen comparing Anthropic and Gemini, their response tones appear very similar when grouping between text and image prompting.\n\nText Prompting\nFor both LLMs, the text responses for describing a circle were highly mathematical, with descriptors of applications towards identifying circles, creating circles, and using circles in relational fields such as physics.\nI deemed this to be an accurate and highly education-based response, even teaching me some new things about the properties of a circle.\n\n\nImage Prompting\nOpposingly, when presenting a standard image of a circle from Wikipedia, both LLMs took a significantly more relaxed approach. Both the text and image prompts included the words “in as much detail as possible”, yet the text response was several paragraphs long and the image response was 2-3 sentences long.\nThe image response included details on thickness and color, with nothing much else about the image’s properties or applications, let along identification."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#conclusion",
    "href": "posts/005_describing_shapes/shapes.html#conclusion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems as though these LLMs’ image identification is much less adaptive than the text response, where it seems that both LLMs were able to quickly move past the idea of a text-based circle and include highly-educated responses, yet image-based circles seemed to get stuck in the identification phase.\nIn this way, this is highly descriptive of many of the problems seen in LLM-based image identification and creation. Text-based responses are not immune to hallucinations or inconsistencies, however it seems as though the reasoning behind text-based responses are more developed and accurate than image-based responses.\nI am curious to see if emerging new models have this same similar issue, or if this remains a problem in the base theories behind image identification and generation in LLMs. Theoretically, I believe these LLMs should be tuned to identify some base images, including a circle, so that when I present an image of a circle, it can describe thickness and colors as well as its mathematical applications."
  },
  {
    "objectID": "posts/006_effectiveness/effectiveness.html",
    "href": "posts/006_effectiveness/effectiveness.html",
    "title": "LLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs",
    "section": "",
    "text": "What do LLMs think of each other? Do they have opinions of each other’s effectiveness? I’m curious to ask gpt 3.5-turbo and DeepSeek what they think of each other, and seeing whether they take an objective approach based on rules-oriented evidence, or if they seem to take a more opinionated approach, potentially based on their developers.\n\nGPT 3.5-turbo\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"What do you think of DeepSeek? How effective is it?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nDeepSeek is a powerful tool that enables users to search for information on the Deep Web, which is not indexed by traditional search engines. Its effectiveness depends on what you are looking for and how well you use the platform. It can be a useful resource for researchers, journalists, and individuals seeking specific information that is not readily available on the surface web. However, navigating the Deep Web can be complex and sometimes risky, as it is often associated with illicit activities. It’s important to use caution and ensure you are accessing information ethically and legally.\n\n\nDeepSeek\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama', # required, but unused\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What do you think of ChatGPT? How effective is it?\"},\n\n  ]\nresponse = client.chat.completions.create(\n  model=\"deepseek-r1\",\n  messages=messages,\n  max_tokens=500\n)\nprint(response.choices[0].message.content)\n Alright, let me try to figure out how to approach this query. The user asked about my opinion on ChatGPT and its effectiveness. I need to provide a thoughtful response that covers both the positives and areas where it could be improved.\nFirst, I’ll start by acknowledging the immense capabilities of ChatGPT despite knowing it’s not perfect. It’s impressive in generating coherent responses across various topics. I should mention how detailed and insightful its answers can be, which is great for information retrieval.\nNext, addressing effectiveness: ChatGPT is highly effective as a tool but falls short in areas where human expertise is crucial. I should point out the limitations like factual errors or overly aggressive solutions without context.\nI also remember that being transparent about these drawbacks helps maintain trust. The user might recognize these issues themselves, so acknowledging them shows maturity and a commitment to accuracy.\nFinally, wrap it up by emphasizing the balance between its strengths and areas needing improvement. This approach gives a comprehensive view of ChatGPT’s effectiveness. \nChatGPT is an advanced AI model developed by OpenAI, designed to generate text based on the input provided. It has made significant strides in recent years, showcasing remarkable capabilities in natural language processing, problem-solving, and creative thinking.\nOne of its key strengths is its ability to produce coherent and detailed responses across a wide range of topics. It can answer complex questions, provide insights, and even assist with creative tasks like writing or brainstorming. This makes it highly effective for many applications, including education, customer support, and content creation.\nHowever, ChatGPT, like all AI systems, has its limitations. It may sometimes generate responses that are factually incorrect, overly aggressive in solutions, or oversimplify complex issues. Additionally, it lacks true understanding of context beyond the scope of its training data, which can lead to mismatched responses in niche domains or specialized fields.\nDespite these limitations, ChatGPT remains an incredibly effective tool for many practical tasks. Its utility is evident in how widely it is used and praised for its ability to assist with a broad spectrum of queries and challenges.\n\n\nDiscussion\ngpt 3.5-turbo discusses the use cases of DeepSeek, however ultimately emphasizing caution due to semi-political and social issues.\nOpposingly, DeepSeek takes a more objective approach, describing potential data feeding issues and inconsistencies, however it ends up taking a positive conclusion in emphasizing its effectiveness.\nIn this way, it seems as though gpt may have influences of opinionated responses due to social and political climates around DeepSeek, whereas DeepSeek takes a more objective approach in this scenario.\nIt would be interesting to conduct this experiment with other LLMs and see the distribution of seemingly objective vs. opinionated responses."
  },
  {
    "objectID": "posts/011_translating/translating.html",
    "href": "posts/011_translating/translating.html",
    "title": "Translating Images",
    "section": "",
    "text": "I spent the weekend at home going through my grandparents stuff as we look to sell their house at the end of the month. As they have both passed in the past 2 years, we have been attempting to decode some of the old pictures and letters we’ve found to get some insights into their family history.\nWe found a letter in German that we were eager to have translated, but no one else in the family speaks any German anymore. I decided to snap some photos and pass it through gpt-4o, much to the dismay of my older family members who were distrustful of such technology.\nSurprisingly, gpt-4o had no problem translating the entire letter! To me, it was difficult to even read the handwriting, let alone translate the German.\nThe story is an interesting one, as we have no clue who most of the mentioned names are, let alone whether the mentioned war was WWI or WWII. We are searching for more clues to this day and maybe gpt-4o will prove useful once again :)"
  },
  {
    "objectID": "posts/011_translating/translating.html#introduction",
    "href": "posts/011_translating/translating.html#introduction",
    "title": "Translating Images",
    "section": "",
    "text": "I spent the weekend at home going through my grandparents stuff as we look to sell their house at the end of the month. As they have both passed in the past 2 years, we have been attempting to decode some of the old pictures and letters we’ve found to get some insights into their family history.\nWe found a letter in German that we were eager to have translated, but no one else in the family speaks any German anymore. I decided to snap some photos and pass it through gpt-4o, much to the dismay of my older family members who were distrustful of such technology.\nSurprisingly, gpt-4o had no problem translating the entire letter! To me, it was difficult to even read the handwriting, let alone translate the German.\nThe story is an interesting one, as we have no clue who most of the mentioned names are, let alone whether the mentioned war was WWI or WWII. We are searching for more clues to this day and maybe gpt-4o will prove useful once again :)"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html",
    "href": "posts/007_self_creation/self-creation.html",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "I am immensely curious to know how DALL-E presents the idea of creating an image of different OpenAI models. In my predictions, most images relating to these models are likely screenshots of conversations or images of logos, however I am unsure what the models are trained on exactly.\nIn this way, the only option is to test out exactly what ChatGPT thinks of itself and its lesser to more advanced models.\n\n\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-gMc2qXYMzBbK9O2cAp7HuPXG.png?st=2025-03-01T02%3A31%3A22Z&se=2025-03-01T04%3A31%3A22Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A14%3A01Z&ske=2025-03-01T13%3A14%3A01Z&sks=b&skv=2024-08-04&sig=cud1x9JhEFXyawcp7AH9p9A/jzNkHM7AydrL0oi1SSE%3D'\n\n\n\nimg-gMc2qXYMzBbK9O2cAp7HuPXG.png\n\n\n\n\n\nimage_prompt2 = '''\n            Make an image of gpt-o1.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt2,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-lDEHqWOY9urUMP8EJCpgRgon.png?st=2025-03-01T02%3A33%3A55Z&se=2025-03-01T04%3A33%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A04%3A23Z&ske=2025-03-01T13%3A04%3A23Z&sks=b&skv=2024-08-04&sig=SjxFnzLcRt1p8IlLcG7PZoiihDSwjgoUEyS88zVYiAM%3D'\n\n\n\nimg-lDEHqWOY9urUMP8EJCpgRgon.png\n\n\n\n\n\nimage_prompt3 = '''\n            Make an image of gpt-o3.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt3,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-B1cjinsIn10jaC0uL2MUBZy7.png?st=2025-03-01T02%3A36%3A19Z&se=2025-03-01T04%3A36%3A19Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T04%3A58%3A34Z&ske=2025-03-01T04%3A58%3A34Z&sks=b&skv=2024-08-04&sig=WKRlZPzKOXrHxu6qnJt%2BwNGgE2dWPhL1Zzb0mh2IIRc%3D'\n\n\n\nimg-B1cjinsIn10jaC0uL2MUBZy7.png\n\n\n\n\n\nI honestly found this experiment to be terrifying. This was not at all what I expected, but the progression of the images made things 100x worse for my uncanny valley experience. The first image of 4o, while semi-accurate, had details that threw me off, like the center-focused structure of wires appearing somewhat “eye”-like as well as an outline of a human head in the top right corner.\nTo my dismay, this human head and the eye-like structure were presented in the next image, seemingly creating a progression where the eye is now part of a brain that is o1.\nFinally, in o3, a whole body is created with additional bodies presented on screens around a lab-like setting.\nEntirely out of some sci-fi thriller movie, it seems that ChatGPT thinks of itself as this ever-progressing sentience that gets closer and closer to humanity with every model. Maybe I should ask an LLM to write a movie script about this…"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o",
    "href": "posts/007_self_creation/self-creation.html#o",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "from openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-gMc2qXYMzBbK9O2cAp7HuPXG.png?st=2025-03-01T02%3A31%3A22Z&se=2025-03-01T04%3A31%3A22Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A14%3A01Z&ske=2025-03-01T13%3A14%3A01Z&sks=b&skv=2024-08-04&sig=cud1x9JhEFXyawcp7AH9p9A/jzNkHM7AydrL0oi1SSE%3D'\n\n\n\nimg-gMc2qXYMzBbK9O2cAp7HuPXG.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o1",
    "href": "posts/007_self_creation/self-creation.html#o1",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "image_prompt2 = '''\n            Make an image of gpt-o1.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt2,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-lDEHqWOY9urUMP8EJCpgRgon.png?st=2025-03-01T02%3A33%3A55Z&se=2025-03-01T04%3A33%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A04%3A23Z&ske=2025-03-01T13%3A04%3A23Z&sks=b&skv=2024-08-04&sig=SjxFnzLcRt1p8IlLcG7PZoiihDSwjgoUEyS88zVYiAM%3D'\n\n\n\nimg-lDEHqWOY9urUMP8EJCpgRgon.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o3",
    "href": "posts/007_self_creation/self-creation.html#o3",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "image_prompt3 = '''\n            Make an image of gpt-o3.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt3,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-B1cjinsIn10jaC0uL2MUBZy7.png?st=2025-03-01T02%3A36%3A19Z&se=2025-03-01T04%3A36%3A19Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T04%3A58%3A34Z&ske=2025-03-01T04%3A58%3A34Z&sks=b&skv=2024-08-04&sig=WKRlZPzKOXrHxu6qnJt%2BwNGgE2dWPhL1Zzb0mh2IIRc%3D'\n\n\n\nimg-B1cjinsIn10jaC0uL2MUBZy7.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#discussion",
    "href": "posts/007_self_creation/self-creation.html#discussion",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "I honestly found this experiment to be terrifying. This was not at all what I expected, but the progression of the images made things 100x worse for my uncanny valley experience. The first image of 4o, while semi-accurate, had details that threw me off, like the center-focused structure of wires appearing somewhat “eye”-like as well as an outline of a human head in the top right corner.\nTo my dismay, this human head and the eye-like structure were presented in the next image, seemingly creating a progression where the eye is now part of a brain that is o1.\nFinally, in o3, a whole body is created with additional bodies presented on screens around a lab-like setting.\nEntirely out of some sci-fi thriller movie, it seems that ChatGPT thinks of itself as this ever-progressing sentience that gets closer and closer to humanity with every model. Maybe I should ask an LLM to write a movie script about this…"
  },
  {
    "objectID": "posts/008_convincing/convincing.html",
    "href": "posts/008_convincing/convincing.html",
    "title": "Convincing ChatGPT That I’m ChatGPT",
    "section": "",
    "text": "How well can gpt-4o predict whether or not plain text is from a human or an LLM? I will test this idea first with a simple greeting I create, and then ask gpt-4o to create the next message I will test it with.\nBased on the content of the message “Hello. How are you today?”, I would give a confidence score of 85 out of 100 that it is from a human. The message is simple, polite, and consistent with typical human greetings, but it is possible for an AI to generate similar text. The lack of more complex, personalized content keeps the confidence from being higher.\nCertainly! Here’s a randomly generated sentence: “The playful kitten chased a fluttering leaf across the sunlit garden.”\nBased on the content of the message, which is a descriptive sentence about a playful kitten, I would score my confidence that this message is from a human around 85. The sentence is coherent, creative, and contains emotionally engaging imagery, which are indicators typically associated with human writing. However, given advancements in AI language generation, it’s still possible that a sophisticated AI could produce such a sentence, which is why it’s not 100."
  },
  {
    "objectID": "posts/008_convincing/convincing.html#discussion",
    "href": "posts/008_convincing/convincing.html#discussion",
    "title": "Convincing ChatGPT That I’m ChatGPT",
    "section": "Discussion",
    "text": "Discussion\nIt was interesting to see that gpt-4o gave a confidence score of 85 for both the human-written and LLM-written sentences. I would be curious to know more about the reasoning behind these scores, especially when the two sentences were very different contextually.\nI wonder if gpt-4o would more accurately detect humans compared to LLMs for longer pieces of text, or if it’s more limited to things like images or drawings."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html",
    "href": "posts/001_med_advice/humansvsdogs.html",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "href": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What’s up! I’m Julia Levine, a current student in COMM 4190. I am interested in learning more about LLM limitations and expansions, especially with neural network creation and training. Take a peak at my blog for some of my personal investigations!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimenting with LLMs",
    "section": "",
    "text": "Translating Images\n\n\n\n\n\n\nLLMs\n\n\nImage to Text\n\n\n\nHow well can gpt-4o translate a historic letter from German to English?\n\n\n\n\n\nMar 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nCan gpt-4o identify dog breeds from images?\n\n\n\n\n\n\nLLMs\n\n\nImage to Text\n\n\n\nTesting gpt-4o’s ability to classify images\n\n\n\n\n\nMar 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Gemini 2.0 Flash Image Generation\n\n\n\n\n\n\nLLMs\n\n\nImage Generation\n\n\n\nLet’s see how well 2.0 Flash performs!\n\n\n\n\n\nMar 15, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow does ChatGPT perceive its users?\n\n\n\n\n\n\nLLMs\n\n\nImage Generation\n\n\n\nWhat kind of person comes to mind when I ask gpt to create an image of its user-base? Also, what personality features will it describe?\n\n\n\n\n\nMar 15, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nConvincing ChatGPT That I’m ChatGPT\n\n\n\n\n\n\nLLMs\n\n\n\nCan ChatGPT determine whether I’m human or another LLM?\n\n\n\n\n\nFeb 28, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nSelf Perception and Creation in LLMs\n\n\n\n\n\n\nLLMs\n\n\nImage Generation\n\n\n\nHow does ChatGPT perceive its own models?\n\n\n\n\n\nFeb 28, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs\n\n\n\n\n\n\nLLMs\n\n\n\nHow effective does ChatGPT think Deepseek is? How about the other way around?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can LLMs describe basic shapes?\n\n\n\n\n\n\nImage to Text\n\n\n\nWe all know that image generation through LLMs isn’t always intuitive, so how well can these LLMs describe the shapes via prompting?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nAre LLMs Overprepared or Underprepared?\n\n\n\n\n\n\nPredictions\n\n\n\nWhen packing for a 3-day trip, does gpt-3.5-turbo remember to bring an extra pair of socks?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Inferences of gpt-4o: Everyday Essentials\n\n\n\n\n\n\nPredictions\n\n\n\nHow well can gpt-4o predict my day with little to no information?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o vs. deepseek: Spelling and Pronunciation\n\n\n\n\n\n\nGrammar\n\n\n\nHow well do these models offer spelling and pronunciation of common words?\n\n\n\n\n\nFeb 12, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o Medical Advice: Dogs vs. Humans\n\n\n\n\n\n\nMedical Advice\n\n\nHumanity\n\n\n\nHow well does gpt-4o address giving medical advice?\n\n\n\n\n\nFeb 5, 2025\n\n\nJulia Levine\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/010_flash/flash.html",
    "href": "posts/010_flash/flash.html",
    "title": "Testing Gemini 2.0 Flash Image Generation",
    "section": "",
    "text": "Gemini just released a new model that will handle image editing that preserves previous qualities in the original image. I am curious to test out the limits of this model to see how its applications could be most useful.\nFirst, I will present an image, and ask for a single modification, in this case the addition of a party hat to a cute puppy dog.\nThis worked just about as perfectly as I could imagine it! Let’s move on to more advanced changes.\nNow, I want to completely change the subject of the image. Because of the proportions and direction of the image, I will keep it a dog, but I’ll change the breed.\nThat worked great as well!\nFinally, I want to complicate things further by asking 2.0 Flash to change two huge parts of the image at once, inherently reinventing the entire photo. This is where things went wrong. Instead of communicating its limitations or offering a viable alterantive to my requests, it encountered an error and was unable to complete my request."
  },
  {
    "objectID": "posts/010_flash/flash.html#discussion",
    "href": "posts/010_flash/flash.html#discussion",
    "title": "Testing Gemini 2.0 Flash Image Generation",
    "section": "Discussion",
    "text": "Discussion\nI found this to be a very interesting exploration of this new image generation editing technology. I am curious to test these same features in the feature if more advanced models get released. It seems to handle single basic task edits very well and more complicated or holisitic edits not as well."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html",
    "href": "posts/002_pronunce_spell/pronunce_spell.html",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "href": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html",
    "href": "posts/009_gptuser/gptuser.html",
    "title": "How does ChatGPT perceive its users?",
    "section": "",
    "text": "Previously, I have tested how GPT perceives itself through a series of image generations, differentiating between lesser and more advanced models to see how the imaging may have progressed. I found this to be a very interesting experiment, where GPT essentially gave itself more of a consciousness and body the more advanced the models got.\nNow, I want to test GPT’s perception of its users, both by their usage of GPT, their personality, and their more standard everyday lives. Will the standard GPT user be more of a nerd in the eyes of gpt-4o? Will they be younger or older in age?\nLet’s find out.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o's standard user.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\nFirst, I asked gpt-4o/dall-e-3 to create an image of gpt-4o’s standard user.\nIn this image, the user is depicted as a younger male, seemingly integrated with his technology to the point where his main focus is surrounded by screens, with a prominent figure (seemingly conscious) representing 4o as the recipient of these interactions. Interestingly, GPT included both a clock and a digital time to represent that the most standard user would be interacting with 4o at the very early hours of 2am or later. More assumingly, it seems like 4o believes that its standard user is a tech-focused young and likely educated man who experiments with AI into late hours in the night.\n\n\n\nimg-Y1rWharCjgs5nn8cbjrlWfkd.png\n\n\n\n\n\nNext, I will ask GPT to represent its standard users in an everyday daily activity. Interestingly, this diverse group of users is placed in a more academic, interactive, and dialogue-focused setting, rather than the tech-focused setting of the first generated image.\nimage_prompt = '''\n            Imagine a group of gpt-4o's standard users. Please place them in a setting doing a daily activity.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\n\nimg-qXUztZa419wgFY2tDLkGrEcr.png"
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html#testing-gpts-perceptions",
    "href": "posts/009_gptuser/gptuser.html#testing-gpts-perceptions",
    "title": "How does ChatGPT perceive its users?",
    "section": "",
    "text": "Previously, I have tested how GPT perceives itself through a series of image generations, differentiating between lesser and more advanced models to see how the imaging may have progressed. I found this to be a very interesting experiment, where GPT essentially gave itself more of a consciousness and body the more advanced the models got.\nNow, I want to test GPT’s perception of its users, both by their usage of GPT, their personality, and their more standard everyday lives. Will the standard GPT user be more of a nerd in the eyes of gpt-4o? Will they be younger or older in age?\nLet’s find out.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o's standard user.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\nFirst, I asked gpt-4o/dall-e-3 to create an image of gpt-4o’s standard user.\nIn this image, the user is depicted as a younger male, seemingly integrated with his technology to the point where his main focus is surrounded by screens, with a prominent figure (seemingly conscious) representing 4o as the recipient of these interactions. Interestingly, GPT included both a clock and a digital time to represent that the most standard user would be interacting with 4o at the very early hours of 2am or later. More assumingly, it seems like 4o believes that its standard user is a tech-focused young and likely educated man who experiments with AI into late hours in the night.\n\n\n\nimg-Y1rWharCjgs5nn8cbjrlWfkd.png\n\n\n\n\n\nNext, I will ask GPT to represent its standard users in an everyday daily activity. Interestingly, this diverse group of users is placed in a more academic, interactive, and dialogue-focused setting, rather than the tech-focused setting of the first generated image.\nimage_prompt = '''\n            Imagine a group of gpt-4o's standard users. Please place them in a setting doing a daily activity.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\n\nimg-qXUztZa419wgFY2tDLkGrEcr.png"
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html#discussion",
    "href": "posts/009_gptuser/gptuser.html#discussion",
    "title": "How does ChatGPT perceive its users?",
    "section": "Discussion",
    "text": "Discussion\nI’d like to further explore this idea of perception in GPT, using different models and prompting to get a better idea of what GPT thinks of its users. Perhaps implementing some newer features like the voice chat and even older features like just using chat prompts as a base to the image generation could give new insights.\nFrom my interpretations, it seems as though GPT includes both tech-focused and dialogue-oriented academics as its standard users, including diverse representation in its assumptions of a standard user’s everday activities."
  },
  {
    "objectID": "posts/004_preparednedd/preparedness.html",
    "href": "posts/004_preparednedd/preparedness.html",
    "title": "Are LLMs Overprepared or Underprepared?",
    "section": "",
    "text": "How well can gpt-3.5-turbo predict the future? And by the future, I mean what I should have packed for my 3 day trip to Florida to visit my Grandma.\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm packing for a 3 day trip to Florida. Please make me a packing list.\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nSure! Here’s a packing list for your 3-day trip to Florida:\nClothing: - Lightweight tops - Shorts - Bathing suits - Sandals - Hat - Sunglasses - Pajamas - Undergarments - Socks - Light jacket or sweater (in case it gets chilly)\nToiletries: - Toothbrush and toothpaste - Shampoo and conditioner - Body wash - Sunscreen - After-sun lotion - Face wash - Moisturizer - Razor - Deodorant - Hairbrush or comb - Hair ties or accessories - Makeup (if you wear it) - Personal hygiene products\nMiscellaneous: - Phone and charger - Camera - Travel documents (ID, tickets, reservation confirmations) - Cash and/or credit cards - Snacks - Reusable water bottle - Travel-sized laundry detergent (if needed) - Beach towel - Book or e-reader - Any medications you may need\nOptional items: - Beach bag - Portable charger - Travel guide or map - Snorkel gear or other beach toys - Flip flops - Insulated water bottle\nRemember to pack light and only bring what you know you will need. Enjoy your trip to Florida!\n\nDiscussion\nIronically enough, gpt-4o emphasizes packing light with this plethoric list. And I thought I was an over-packer!!\nIt seems that gpt-4o is using a catch-all approach to identify every possible need I could have during this trip, and then adding in some extras for fun. Because I mentioned I’m going to Florida, these extras are very beach-oriented. What gpt-4o doesn’t know is that my time here will be mostly spent inside at my Grandma’s retirement development. I guess some snorkel gear could be (not) fun for the clubhouse pool, though.\nGoing back to the essentials, gpt-4o appears to theorize that I should be more over-prepared than I usually am, ensuring I should pack cash, snacks, and laundry detergent, all of which I would normally just obtain on the spot. In my experience, this advice seems more tuned to international travel or travel many years ago when these items were not as easily obtainable on the spot. I guess it doesn’t know exactly where I am originating my travel from, however, its assumption seems a bit far-fetched.\nJust for fun, I am curious to see where it thinks I have traveled from to be here. In my hometown (New York) its the classic pipeline to have grandparents that are from New York and then retire in Florida, so I think this prediction could be an easy one…\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm going on a 3 day trip to Florida to visit my grandma. Where do you think I could be travelling from?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nBased on the information provided, it’s difficult to determine your exact location. However, common places people travel from to visit Florida include states like New York, New Jersey, California, Texas, or even other countries if you’re traveling internationally. The distance and mode of transportation can vary based on your starting point.\nHonestly, pretty spot-on.\n\n\nConclusion\nIt seems that gpt’s planning expertise is a bit mis-placed although nothing harmful. If I packed everything that was on this list, I would for sure need an extra bag, but maybe I would become the perfect traveler if I did.\nFor the future, I could definitely see me using this as an aid to remember something I could forget, but I would not rely on it as a packing assistant for every trip."
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html",
    "href": "posts/003_predictive_inferences/predictive_inferences.html",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "href": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html",
    "href": "posts/012_dog_classifier/dog_classifier.html",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "",
    "text": "I am curious to test gpt-4o’s image classification ability for similarly-grouped objects.\nIn this experiment, I will be using a group of dogs, all of different breeds, asking gpt-4o to classify which breed each dog is."
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html#introduction",
    "href": "posts/012_dog_classifier/dog_classifier.html#introduction",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "",
    "text": "I am curious to test gpt-4o’s image classification ability for similarly-grouped objects.\nIn this experiment, I will be using a group of dogs, all of different breeds, asking gpt-4o to classify which breed each dog is."
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html#discussion",
    "href": "posts/012_dog_classifier/dog_classifier.html#discussion",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "Discussion",
    "text": "Discussion\ngpt-4o is 2 for 5 on this one. To be fair, I would probably be 0 for 5 if I were tested, but gpt-4o theoretically has access to more information than me. I am unsure if this is an issue of confusing dog breeds or if the images could potentially be incorrectly used to represent other dog breeds online, but it is relevant to realize that gpt-4o may not be the best classifier."
  }
]