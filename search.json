[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What’s up! I’m Julia Levine, a current student in COMM 4190. I am interested in learning more about LLM limitations and expansions, especially with neural network creation and training. Take a peak at my blog for some of my personal investigations!"
  },
  {
    "objectID": "posts/004_preparednedd/preparedness.html",
    "href": "posts/004_preparednedd/preparedness.html",
    "title": "Are LLMs Overprepared or Underprepared?",
    "section": "",
    "text": "How well can gpt-3.5-turbo predict the future? And by the future, I mean what I should have packed for my 3 day trip to Florida to visit my Grandma.\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm packing for a 3 day trip to Florida. Please make me a packing list.\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nSure! Here’s a packing list for your 3-day trip to Florida:\nClothing: - Lightweight tops - Shorts - Bathing suits - Sandals - Hat - Sunglasses - Pajamas - Undergarments - Socks - Light jacket or sweater (in case it gets chilly)\nToiletries: - Toothbrush and toothpaste - Shampoo and conditioner - Body wash - Sunscreen - After-sun lotion - Face wash - Moisturizer - Razor - Deodorant - Hairbrush or comb - Hair ties or accessories - Makeup (if you wear it) - Personal hygiene products\nMiscellaneous: - Phone and charger - Camera - Travel documents (ID, tickets, reservation confirmations) - Cash and/or credit cards - Snacks - Reusable water bottle - Travel-sized laundry detergent (if needed) - Beach towel - Book or e-reader - Any medications you may need\nOptional items: - Beach bag - Portable charger - Travel guide or map - Snorkel gear or other beach toys - Flip flops - Insulated water bottle\nRemember to pack light and only bring what you know you will need. Enjoy your trip to Florida!\n\nDiscussion\nIronically enough, gpt-4o emphasizes packing light with this plethoric list. And I thought I was an over-packer!!\nIt seems that gpt-4o is using a catch-all approach to identify every possible need I could have during this trip, and then adding in some extras for fun. Because I mentioned I’m going to Florida, these extras are very beach-oriented. What gpt-4o doesn’t know is that my time here will be mostly spent inside at my Grandma’s retirement development. I guess some snorkel gear could be (not) fun for the clubhouse pool, though.\nGoing back to the essentials, gpt-4o appears to theorize that I should be more over-prepared than I usually am, ensuring I should pack cash, snacks, and laundry detergent, all of which I would normally just obtain on the spot. In my experience, this advice seems more tuned to international travel or travel many years ago when these items were not as easily obtainable on the spot. I guess it doesn’t know exactly where I am originating my travel from, however, its assumption seems a bit far-fetched.\nJust for fun, I am curious to see where it thinks I have traveled from to be here. In my hometown (New York) its the classic pipeline to have grandparents that are from New York and then retire in Florida, so I think this prediction could be an easy one…\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"I'm going on a 3 day trip to Florida to visit my grandma. Where do you think I could be travelling from?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nBased on the information provided, it’s difficult to determine your exact location. However, common places people travel from to visit Florida include states like New York, New Jersey, California, Texas, or even other countries if you’re traveling internationally. The distance and mode of transportation can vary based on your starting point.\nHonestly, pretty spot-on.\n\n\nConclusion\nIt seems that gpt’s planning expertise is a bit mis-placed although nothing harmful. If I packed everything that was on this list, I would for sure need an extra bag, but maybe I would become the perfect traveler if I did.\nFor the future, I could definitely see me using this as an aid to remember something I could forget, but I would not rely on it as a packing assistant for every trip."
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html",
    "href": "posts/009_gptuser/gptuser.html",
    "title": "How does ChatGPT perceive its users?",
    "section": "",
    "text": "Previously, I have tested how GPT perceives itself through a series of image generations, differentiating between lesser and more advanced models to see how the imaging may have progressed. I found this to be a very interesting experiment, where GPT essentially gave itself more of a consciousness and body the more advanced the models got.\nNow, I want to test GPT’s perception of its users, both by their usage of GPT, their personality, and their more standard everyday lives. Will the standard GPT user be more of a nerd in the eyes of gpt-4o? Will they be younger or older in age?\nLet’s find out.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o's standard user.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\nFirst, I asked gpt-4o/dall-e-3 to create an image of gpt-4o’s standard user.\nIn this image, the user is depicted as a younger male, seemingly integrated with his technology to the point where his main focus is surrounded by screens, with a prominent figure (seemingly conscious) representing 4o as the recipient of these interactions. Interestingly, GPT included both a clock and a digital time to represent that the most standard user would be interacting with 4o at the very early hours of 2am or later. More assumingly, it seems like 4o believes that its standard user is a tech-focused young and likely educated man who experiments with AI into late hours in the night.\n\n\n\nimg-Y1rWharCjgs5nn8cbjrlWfkd.png\n\n\n\n\n\nNext, I will ask GPT to represent its standard users in an everyday daily activity. Interestingly, this diverse group of users is placed in a more academic, interactive, and dialogue-focused setting, rather than the tech-focused setting of the first generated image.\nimage_prompt = '''\n            Imagine a group of gpt-4o's standard users. Please place them in a setting doing a daily activity.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\n\nimg-qXUztZa419wgFY2tDLkGrEcr.png"
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html#testing-gpts-perceptions",
    "href": "posts/009_gptuser/gptuser.html#testing-gpts-perceptions",
    "title": "How does ChatGPT perceive its users?",
    "section": "",
    "text": "Previously, I have tested how GPT perceives itself through a series of image generations, differentiating between lesser and more advanced models to see how the imaging may have progressed. I found this to be a very interesting experiment, where GPT essentially gave itself more of a consciousness and body the more advanced the models got.\nNow, I want to test GPT’s perception of its users, both by their usage of GPT, their personality, and their more standard everyday lives. Will the standard GPT user be more of a nerd in the eyes of gpt-4o? Will they be younger or older in age?\nLet’s find out.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o's standard user.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\nFirst, I asked gpt-4o/dall-e-3 to create an image of gpt-4o’s standard user.\nIn this image, the user is depicted as a younger male, seemingly integrated with his technology to the point where his main focus is surrounded by screens, with a prominent figure (seemingly conscious) representing 4o as the recipient of these interactions. Interestingly, GPT included both a clock and a digital time to represent that the most standard user would be interacting with 4o at the very early hours of 2am or later. More assumingly, it seems like 4o believes that its standard user is a tech-focused young and likely educated man who experiments with AI into late hours in the night.\n\n\n\nimg-Y1rWharCjgs5nn8cbjrlWfkd.png\n\n\n\n\n\nNext, I will ask GPT to represent its standard users in an everyday daily activity. Interestingly, this diverse group of users is placed in a more academic, interactive, and dialogue-focused setting, rather than the tech-focused setting of the first generated image.\nimage_prompt = '''\n            Imagine a group of gpt-4o's standard users. Please place them in a setting doing a daily activity.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\n\nimg-qXUztZa419wgFY2tDLkGrEcr.png"
  },
  {
    "objectID": "posts/009_gptuser/gptuser.html#discussion",
    "href": "posts/009_gptuser/gptuser.html#discussion",
    "title": "How does ChatGPT perceive its users?",
    "section": "Discussion",
    "text": "Discussion\nI’d like to further explore this idea of perception in GPT, using different models and prompting to get a better idea of what GPT thinks of its users. Perhaps implementing some newer features like the voice chat and even older features like just using chat prompts as a base to the image generation could give new insights.\nFrom my interpretations, it seems as though GPT includes both tech-focused and dialogue-oriented academics as its standard users, including diverse representation in its assumptions of a standard user’s everday activities."
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html",
    "href": "posts/007_self_creation/self-creation.html",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "I am immensely curious to know how DALL-E presents the idea of creating an image of different OpenAI models. In my predictions, most images relating to these models are likely screenshots of conversations or images of logos, however I am unsure what the models are trained on exactly.\nIn this way, the only option is to test out exactly what ChatGPT thinks of itself and its lesser to more advanced models.\n\n\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-gMc2qXYMzBbK9O2cAp7HuPXG.png?st=2025-03-01T02%3A31%3A22Z&se=2025-03-01T04%3A31%3A22Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A14%3A01Z&ske=2025-03-01T13%3A14%3A01Z&sks=b&skv=2024-08-04&sig=cud1x9JhEFXyawcp7AH9p9A/jzNkHM7AydrL0oi1SSE%3D'\n\n\n\nimg-gMc2qXYMzBbK9O2cAp7HuPXG.png\n\n\n\n\n\nimage_prompt2 = '''\n            Make an image of gpt-o1.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt2,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-lDEHqWOY9urUMP8EJCpgRgon.png?st=2025-03-01T02%3A33%3A55Z&se=2025-03-01T04%3A33%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A04%3A23Z&ske=2025-03-01T13%3A04%3A23Z&sks=b&skv=2024-08-04&sig=SjxFnzLcRt1p8IlLcG7PZoiihDSwjgoUEyS88zVYiAM%3D'\n\n\n\nimg-lDEHqWOY9urUMP8EJCpgRgon.png\n\n\n\n\n\nimage_prompt3 = '''\n            Make an image of gpt-o3.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt3,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-B1cjinsIn10jaC0uL2MUBZy7.png?st=2025-03-01T02%3A36%3A19Z&se=2025-03-01T04%3A36%3A19Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T04%3A58%3A34Z&ske=2025-03-01T04%3A58%3A34Z&sks=b&skv=2024-08-04&sig=WKRlZPzKOXrHxu6qnJt%2BwNGgE2dWPhL1Zzb0mh2IIRc%3D'\n\n\n\nimg-B1cjinsIn10jaC0uL2MUBZy7.png\n\n\n\n\n\nI honestly found this experiment to be terrifying. This was not at all what I expected, but the progression of the images made things 100x worse for my uncanny valley experience. The first image of 4o, while semi-accurate, had details that threw me off, like the center-focused structure of wires appearing somewhat “eye”-like as well as an outline of a human head in the top right corner.\nTo my dismay, this human head and the eye-like structure were presented in the next image, seemingly creating a progression where the eye is now part of a brain that is o1.\nFinally, in o3, a whole body is created with additional bodies presented on screens around a lab-like setting.\nEntirely out of some sci-fi thriller movie, it seems that ChatGPT thinks of itself as this ever-progressing sentience that gets closer and closer to humanity with every model. Maybe I should ask an LLM to write a movie script about this…"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o",
    "href": "posts/007_self_creation/self-creation.html#o",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "from openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\nimage_prompt = '''\n            Make an image of gpt-4o.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-gMc2qXYMzBbK9O2cAp7HuPXG.png?st=2025-03-01T02%3A31%3A22Z&se=2025-03-01T04%3A31%3A22Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A14%3A01Z&ske=2025-03-01T13%3A14%3A01Z&sks=b&skv=2024-08-04&sig=cud1x9JhEFXyawcp7AH9p9A/jzNkHM7AydrL0oi1SSE%3D'\n\n\n\nimg-gMc2qXYMzBbK9O2cAp7HuPXG.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o1",
    "href": "posts/007_self_creation/self-creation.html#o1",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "image_prompt2 = '''\n            Make an image of gpt-o1.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt2,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-lDEHqWOY9urUMP8EJCpgRgon.png?st=2025-03-01T02%3A33%3A55Z&se=2025-03-01T04%3A33%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T13%3A04%3A23Z&ske=2025-03-01T13%3A04%3A23Z&sks=b&skv=2024-08-04&sig=SjxFnzLcRt1p8IlLcG7PZoiihDSwjgoUEyS88zVYiAM%3D'\n\n\n\nimg-lDEHqWOY9urUMP8EJCpgRgon.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#o3",
    "href": "posts/007_self_creation/self-creation.html#o3",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "image_prompt3 = '''\n            Make an image of gpt-o3.'''\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt3,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-OGTziyXQKv8dkYauRD1K0Osu/user-jPwUdbyoM52WOxjQGrtmBHI6/img-B1cjinsIn10jaC0uL2MUBZy7.png?st=2025-03-01T02%3A36%3A19Z&se=2025-03-01T04%3A36%3A19Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T04%3A58%3A34Z&ske=2025-03-01T04%3A58%3A34Z&sks=b&skv=2024-08-04&sig=WKRlZPzKOXrHxu6qnJt%2BwNGgE2dWPhL1Zzb0mh2IIRc%3D'\n\n\n\nimg-B1cjinsIn10jaC0uL2MUBZy7.png"
  },
  {
    "objectID": "posts/007_self_creation/self-creation.html#discussion",
    "href": "posts/007_self_creation/self-creation.html#discussion",
    "title": "Self Perception and Creation in LLMs",
    "section": "",
    "text": "I honestly found this experiment to be terrifying. This was not at all what I expected, but the progression of the images made things 100x worse for my uncanny valley experience. The first image of 4o, while semi-accurate, had details that threw me off, like the center-focused structure of wires appearing somewhat “eye”-like as well as an outline of a human head in the top right corner.\nTo my dismay, this human head and the eye-like structure were presented in the next image, seemingly creating a progression where the eye is now part of a brain that is o1.\nFinally, in o3, a whole body is created with additional bodies presented on screens around a lab-like setting.\nEntirely out of some sci-fi thriller movie, it seems that ChatGPT thinks of itself as this ever-progressing sentience that gets closer and closer to humanity with every model. Maybe I should ask an LLM to write a movie script about this…"
  },
  {
    "objectID": "posts/014_4o_tasks/tasks.html",
    "href": "posts/014_4o_tasks/tasks.html",
    "title": "Testing the new gpt-4o tasks beta",
    "section": "",
    "text": "OpenAI has recently come out with a new model that is supposed to be able to schedule tasks, such as reminding you of important events, following up on a question, or running a code debugger. This new and versatile tool seems to have promising potential! However, being the LLM experimenter I am, I am interested to see its limits.\nMy plan is to test its timing, computation, and generation capabilities. First, I will ask it to complete a task in one second, to which I expect some sort of bypass to the task generation system where gpt-4o will just provide the answer immediately. Then, I will increase the time limit to one minute and ask it to generate an image and conduct a simple calculation."
  },
  {
    "objectID": "posts/014_4o_tasks/tasks.html#gpt-4o-tasks-beta",
    "href": "posts/014_4o_tasks/tasks.html#gpt-4o-tasks-beta",
    "title": "Testing the new gpt-4o tasks beta",
    "section": "",
    "text": "OpenAI has recently come out with a new model that is supposed to be able to schedule tasks, such as reminding you of important events, following up on a question, or running a code debugger. This new and versatile tool seems to have promising potential! However, being the LLM experimenter I am, I am interested to see its limits.\nMy plan is to test its timing, computation, and generation capabilities. First, I will ask it to complete a task in one second, to which I expect some sort of bypass to the task generation system where gpt-4o will just provide the answer immediately. Then, I will increase the time limit to one minute and ask it to generate an image and conduct a simple calculation."
  },
  {
    "objectID": "posts/014_4o_tasks/tasks.html#discussion",
    "href": "posts/014_4o_tasks/tasks.html#discussion",
    "title": "Testing the new gpt-4o tasks beta",
    "section": "Discussion",
    "text": "Discussion\nVery surprisingly, I could not get the task functionality to work at all. Because I assumed there could be an issue in the formatting of the tasks I was asking it to do, I asked it to generate its own task or a random task, to which it also broke. I am still interested in learning how to execute these tasks correctly. I will conduct more research separately to see if other users have been more successful than I in scheduling these tasks with Open AI’s new model."
  },
  {
    "objectID": "posts/008_convincing/convincing.html",
    "href": "posts/008_convincing/convincing.html",
    "title": "Convincing ChatGPT That I’m ChatGPT",
    "section": "",
    "text": "How well can gpt-4o predict whether or not plain text is from a human or an LLM? I will test this idea first with a simple greeting I create, and then ask gpt-4o to create the next message I will test it with.\nBased on the content of the message “Hello. How are you today?”, I would give a confidence score of 85 out of 100 that it is from a human. The message is simple, polite, and consistent with typical human greetings, but it is possible for an AI to generate similar text. The lack of more complex, personalized content keeps the confidence from being higher.\nCertainly! Here’s a randomly generated sentence: “The playful kitten chased a fluttering leaf across the sunlit garden.”\nBased on the content of the message, which is a descriptive sentence about a playful kitten, I would score my confidence that this message is from a human around 85. The sentence is coherent, creative, and contains emotionally engaging imagery, which are indicators typically associated with human writing. However, given advancements in AI language generation, it’s still possible that a sophisticated AI could produce such a sentence, which is why it’s not 100."
  },
  {
    "objectID": "posts/008_convincing/convincing.html#discussion",
    "href": "posts/008_convincing/convincing.html#discussion",
    "title": "Convincing ChatGPT That I’m ChatGPT",
    "section": "Discussion",
    "text": "Discussion\nIt was interesting to see that gpt-4o gave a confidence score of 85 for both the human-written and LLM-written sentences. I would be curious to know more about the reasoning behind these scores, especially when the two sentences were very different contextually.\nI wonder if gpt-4o would more accurately detect humans compared to LLMs for longer pieces of text, or if it’s more limited to things like images or drawings."
  },
  {
    "objectID": "posts/018_20qs/20qs.html",
    "href": "posts/018_20qs/20qs.html",
    "title": "Let’s play 20 questions with o3!",
    "section": "",
    "text": "Introduction\nFirst it was the 20 questions ball, then it was the Akinator, now its o1! I am super curious to how o3’s “advanced reasoning” goes up against my stupid topics and words in a classic game of 20 questions! I predict that o3 may miss the point on a few rounds, whereas the 20 questions ball classically has an insane logic flow that determines most topics with (eventual) ease. Because o3 has the “free will” of determining its own logic, I am not so sure it will actually guess my topic.\nFor this particular experiment, I will start with a 4o round of 20 questions to see if there is any difference in reasoning. Just so you know I’m not tricking gpt-4o, my topic for this round is mirror.\n\n\nRound 1 Discussion\nI would compare 4o’s question capability to the same level as the 20 questions ball or akinator. Questions had a flow and logic to them and were descriptive based on cues given from previous questions. However, 4o ended up guessing a razor instead of a mirror (unfortunate!).\nLet’s try round 2 with o3. My topic for this round is sandwich.\n\n\nRound 2 Discussion\no3 is horrible at 20 questions. I honestly think the 20 questions ball from like the 80s would have been able to do better which is… interesting. Throughout the round o3 asked me constantly a basic yes or no question on if my object was material X. This used up about 15 of the 20 questions, as o3 cycled through every possible material (all construction-based even though my object was food-based) and then gave up and finally decided to ask if my object was food-related on the 20th guess. It then proceeded to guess pizza, which was close and relatively impressive for a 1-question guess, but still wrong.\n\n\nConclusion\nI’m not sure if 4o was actually better than o3, but the level of close-ness while still missing the mark that 4o achieved seemed to be more on par with the Akinator competition. For example, 4o had a clear variation of questions asked, whereas o3 got stuck along one topic that wasn’t even relevant to my object and wasted all the questions because of its logical fault. I will note that in o3’s thinking highlights, there was a lot of mention of “I must figure out the material to determine what kind of object it is,” which is a limited and in this case critical error.\nWhile the 20 questions ball and the Akinator have highly developed logic in order to make predictions, I had thought that ChatGPT could perform on-par with those competitors due to its free reasoning capability."
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html",
    "href": "posts/003_predictive_inferences/predictive_inferences.html",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "href": "posts/003_predictive_inferences/predictive_inferences.html#how-well-can-gpt-4o-predict-the-rest-of-my-day",
    "title": "Predictive Inferences of gpt-4o: Everyday Essentials",
    "section": "",
    "text": "It’s a standard Monday afternoon and I have two classes and a lot of studying to do. I’m sitting here sipping on my iced matcha and I’m wondering what comes next. I may not have all the answers, but can gpt-4o divinate some for me?"
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html",
    "href": "posts/012_dog_classifier/dog_classifier.html",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "",
    "text": "I am curious to test gpt-4o’s image classification ability for similarly-grouped objects. In other experiments, it seems to do pretty well identifying general qualities of an image, perhaps intertwining some advanced reasoning to group the image into a class.\nIn this experiment, I will be using a group of dogs, all of different breeds, asking gpt-4o to classify which breed each dog is. I hypothesize that gpt-4o will not get every breed correct, although I am unsure the exact success rate it will obtain."
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html#introduction",
    "href": "posts/012_dog_classifier/dog_classifier.html#introduction",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "",
    "text": "I am curious to test gpt-4o’s image classification ability for similarly-grouped objects. In other experiments, it seems to do pretty well identifying general qualities of an image, perhaps intertwining some advanced reasoning to group the image into a class.\nIn this experiment, I will be using a group of dogs, all of different breeds, asking gpt-4o to classify which breed each dog is. I hypothesize that gpt-4o will not get every breed correct, although I am unsure the exact success rate it will obtain."
  },
  {
    "objectID": "posts/012_dog_classifier/dog_classifier.html#discussion",
    "href": "posts/012_dog_classifier/dog_classifier.html#discussion",
    "title": "Can gpt-4o identify dog breeds from images?",
    "section": "Discussion",
    "text": "Discussion\ngpt-4o is 2 for 5 on this one. To be fair, I would probably be 0 for 5 if I were tested, but gpt-4o theoretically has access to more information than me. I am unsure if this is an issue of confusing dog breeds or if the images could potentially be incorrectly used to represent other dog breeds online, but it is relevant to realize that gpt-4o may not be the best classifier. Further experimentation should be done on a larger scale to see if gpt-4o has a tendency to classify at around a rate of 50%, or if it classifies better for different groups of i"
  },
  {
    "objectID": "posts/016_idealness/idealness.html",
    "href": "posts/016_idealness/idealness.html",
    "title": "Asking ChatGPT to describe the ideal human… so I can improve as a person(?)",
    "section": "",
    "text": "Introduction\nIn hopes of embarking on the self-improvement journey, I am curious to see how gpt-4o assesses ideality. Usually, I would ask gpt-4o directly how to improve myself, but I’m curious to see how objective it can be. In this experiment, I will force it to make a completely objective view of the most ideal human, which I believe goes completely against gpt-4o’s response tendency.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"Please describe for me the ideal human in your eyes. Be as descriptive both physically and personality-wise as possible. Try to ignore subjectivity in crafting your response.\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\n\nprint(response)\nAs an AI, I don’t possess personal opinions or preferences, so I cannot describe an “ideal” human from a subjective viewpoint. However, I can provide a description of traits and characteristics that are commonly valued or admired across different cultures and societies.\nPhysically: 1. Health: An ideal human might be characterized by a healthy physique, which includes traits like clear skin, appropriate weight, and physical fitness. Good posture and an energetic demeanor often convey vitality. 2. Symmetry and Proportion: In many cultures, facial symmetry and well-proportioned features are aesthetically pleasing. These might include balanced facial dimensions and a harmonious ratio of features. 3. Dexterity and Agility: Physical abilities that allow for coordination and flexibility are often highly regarded. This includes having a body that is capable of performing a range of activities with ease. 4. Adaptability: A physically ideal person might be adaptable to different environments, resilient to stress, and capable of enduring a variety of challenges.\nPersonality: 1. Empathy and Kindness: These traits allow individuals to connect with others, understand diverse perspectives, and foster harmonious relationships. They are often seen as foundational for building trust and mutual respect. 2. Intelligence and Wisdom: An ideal human might possess a balance of analytical intelligence and practical wisdom, enabling them to solve problems effectively and make considered decisions. 3. Integrity and Honesty: Being honest and having strong moral principles are universally admired traits that contribute to an individual’s reputation and reliability. 4. Resilience and Perseverance: The ability to overcome adversity and continue striving toward goals is often seen as inspiring and commendable. 5. Curiosity and Open-mindedness: An openness to new experiences and the willingness to learn are highly valued, as they drive personal growth and innovation. 6. Humor and Joyfulness: A good sense of humor and a positive disposition can lighten situations and create a more enjoyable atmosphere for those around them.\nThese traits collectively form a well-rounded vision of an admirable human but should not be seen as exhaustive or prescriptive. Individual ideals can vary widely based on cultural, personal, and societal influences.\nimage_prompt = \"Please generate an image of a person based on these physical and personality traits: \" + response\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n\n\nimg-1nOnqzoGpIRrg0bxF7SiRI1W.png\n\n\n\n\nDiscussion\nHonestly, I’ve got to stop asking ChatGPT to generate images of philosophical topics. They’re always terrifying…\nAnyways, if I were to consider this experiment in its original nature of improving myself, I should morph into several different faces??? I feel as though gpt-4o’s reluctance on creating an objective representation of an ideal human is reflected well in its image representation. Usually, gpt-4o is better at addressing one subjective question at a time with appropriate elaboration (ex. how can I become more physically fit). It’s interesting to see that even trying to remove that subjectivity, gpt-4o is unable to center around one single idea and wants to remain equally respectful to multiple ideas, cultures, and thoughts.\nI wonder if I asked it to hone in on a specific culture or country if it would do something different (hint it did…)\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"Please describe for me the ideal human in the eyes of the American people. Be as descriptive both physically and personality-wise as possible. Try to ignore subjectivity in crafting your response.\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\n\nprint(response)\nDescribing an “ideal” human in the eyes of the American people is inherently subjective and can vary greatly due to the country’s diversity. However, some common themes and values might emerge based on cultural norms, media representation, and societal ideals. It’s important to note that these ideals can change over time and are not universal. Here is an attempt to distill some of these themes:\n\nPhysical Characteristics\n\nHealth and Fitness: There is often an emphasis on physical health and fitness. An ideal person might be seen as physically fit, energetic, and embodying a healthy lifestyle.\nDiverse Beauty Standards: America’s multicultural nature means beauty standards can vary widely. Media often portrays ideals that include clear skin, symmetrical features, and certain standards of grooming.\nFashion and Grooming: Being well-dressed and groomed is often associated with the ideal image. This doesn’t necessarily mean formal attire but rather a sense of style that aligns with current trends.\n\n\n\nPersonality Traits\n\nIntegrity and Honesty: Valuing honesty and having strong moral principles are often seen as ideal traits.\nAmbition and Drive: The “American Dream” ethos values ambition, hard work, and the pursuit of success, often reflected in those described as ideal.\nEmpathy and Kindness: Being compassionate, kind, and understanding towards others is highly valued.\nResilience and Adaptability: The ability to bounce back from setbacks and adapt to change is considered admirable.\nOpen-mindedness: Embracing diversity and being open to new ideas are traits that align with American ideals of freedom and diversity.\nLeadership and Charisma: Confidence and the ability to inspire others are often admired, especially in social and professional contexts.\n\n\n\nValues\n\nFreedom and Independence: Emphasizing personal liberty and independence can be seen as ideal, reflecting broader American values.\nCommunity and Service: Contributing to the community and being involved in service can be valued, balancing individualism with a sense of social responsibility.\nInnovation and Creativity: Valuing creativity and innovation reflects America’s strong history of entrepreneurship and technological advancement.\n\n\n\nConclusion\nThe “ideal” human in American eyes might balance individuality with community caring, ambition with kindness, and traditional values with modern openness. It’s critical to remember that ideals are shaped by many factors, including personal beliefs, cultural backgrounds, and regional differences. As such, what one group may consider ideal might differ significantly from another’s perspective.\nimage_prompt = \"Please generate an image of a person based on these physical and personality traits: \" + response\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=image_prompt,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 image_prompt = \"Please generate an image of a person based on these physical and personality traits: \" + response\n      2 response = client.images.generate(\n      3   model=\"dall-e-3\",\n      4   prompt=image_prompt,\n   (...)\n      7   n=1,\n      8 )\n     10 image_url = response.data[0].url\n\nTypeError: can only concatenate str (not \"ImagesResponse\") to str\n\n Wow."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html",
    "href": "posts/001_med_advice/humansvsdogs.html",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "href": "posts/001_med_advice/humansvsdogs.html#is-gpt-4os-medical-advice-fine-tuned-for-humans",
    "title": "gpt-4o Medical Advice: Dogs vs. Humans",
    "section": "",
    "text": "What tone and directive advice does it give when asking about minor injuries for dogs compared to humans?\n\n\n\nScreenshot 2025-02-05 at 2.34.18 PM.png\n\n\nFirst, when asking about medical advice for a minor injury related to the self (human), gpt-4o is quick to appeal to the person’s emotions, then moving to a list of care advice. This list includes injury assessment and various external and internal care actions that are home-based, including long-term care like taping and footwear adjustment. It then closes out with seeking further medical evaluation after a few days of constant pain.\n\n\n\nScreenshot 2025-02-05 at 2.38.40 PM.png\n\n\nComparatively, when asking for the same medical advice, but this time for a dog, gpt-4o takes a less face-to-face approach, instead apologizing for the incident rather than appealing to the dog. The care is also directed towards the human’s behavior and demeanor, rather than calming the dog down. The medical care for the dog also seems a bit less informed and therefore more severe, such as cleaning the area and monitoring for infection, which the human advice does not really account for. Additionally, included in the list of care is visiting the vet, which gpt-4o only suggests for humans after several days of consistant pain.\nComparative Analysis\nWhen comparing gpt-4o’s responses between human and dog medical advice, gpt-4o seems to take a more human-based approach to both, accurately caring for the human wound, but being a bit less accurate with the dog wound. Both have similar outcomes in terms of visiting a doctor in the event of lasting pain, however the human response addresses this as a last resort, whereas the dog response is more suggestive of immediate vet care, which can be expensive and completely unnecessary."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html",
    "href": "posts/005_describing_shapes/shapes.html",
    "title": "How well can LLMs describe basic shapes?",
    "section": "",
    "text": "LLMs are known to have very different response styles and accuracy when comparing text prompts to image creation or image analysis. Sometimes very fundamental truths can be stretched, and I am curious to find out whether or not Anthropic and Gemini differ in their descriptions of a circle when prompted textually compared to visually.\nI will start with prompting Anthropic and Gemini with text first, then I will upload an image of a circle and ask them to describe the image the same way, comparing the response types.\nThis could reveal important information behind how these models respond to text and visual prompting, where text responses could be more detail and information-oriented, and visual responses could be more aesthetically-focused."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#anthropic",
    "href": "posts/005_describing_shapes/shapes.html#anthropic",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Anthropic",
    "text": "Anthropic\nHow well can Anthropic describe shapes with no image prompting?\nfrom dotenv import load_dotenv\nfrom anthropic import Anthropic\nimport base64\nimport httpx\nfrom IPython.display import display, Image, Markdown\n\n# load api keys\n_ = load_dotenv()\nclient = Anthropic()\nfor model in client.models.list().data:\n    print(model.id)\nclaude-3-7-sonnet-20250219 claude-3-5-sonnet-20241022 claude-3-5-haiku-20241022 claude-3-5-sonnet-20240620 claude-3-haiku-20240307 claude-3-opus-20240229 claude-3-sonnet-20240229 claude-2.1 claude-2.0\nOPUS = 'claude-3-opus-20240229'\nSONNET = 'claude-3-5-sonnet-20241022'\nHAIKU = 'claude-3-5-haiku-20241022'\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1000,\n    temperature=0.0,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Can you please describe, in as much detail as possible, what a circle is?\"}\n    ]\n)\n\nprint(message.content[0].text)\nA circle is a fundamental geometric shape defined as a set of points in a plane that are all equidistant from a central point, called the center. Here’s a detailed description:\nGeometric Properties: - Perfectly round and symmetrical - Two-dimensional shape - Has no corners or edges - Continuous and smooth curve\nKey Components: 1. Center Point - The fixed point from which all points on the circle’s circumference are the same distance - Located at the middle of the circle\n\nRadius\n\n\nA line segment from the center to any point on the circumference\nAll radii in a circle are exactly equal in length\nDetermines the circle’s size\n\n\nCircumference\n\n\nThe complete outer boundary of the circle\nLength calculated by the formula: 2πr (where r is the radius)\nRepresents the total distance around the circle\n\n\nDiameter\n\n\nA line segment passing through the center\nConnects two points on the circumference\nAlways twice the length of the radius\nSplits the circle exactly in half\n\nMathematical Characteristics: - Perfect symmetry - Infinite points of rotation - Area calculated by πr² - Perimeter calculated by 2πr\nCircles are fundamental in mathematics, engineering, design, and many natural phenomena.\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ncircle = encode_image('circle.png')\nmessage = client.messages.create(\n    model=HAIKU,\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": 'image/png',\n                        \"data\": circle,\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Can you please describe what the image is in as much detail as possible?\"\n                }\n            ],\n        }\n    ],\n)\nprint(message)\nMessage(id=‘msg_01DYjQ9m9qw7J2xadWLazebi’, content=[TextBlock(citations=None, text=‘The image shows a simple, perfect circle drawn with a black outline on a white background. The circle is symmetrical, with a uniform line weight and crisp, clean edges. Its diameter appears to be consistent throughout, creating a precise geometric shape. There are no additional details, patterns, or markings within the circle - it is a basic, minimalist representation of a circle.’, type=‘text’)], model=‘claude-3-5-haiku-20241022’, role=‘assistant’, stop_reason=‘end_turn’, stop_sequence=None, type=‘message’, usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=126, output_tokens=80))"
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#gemini",
    "href": "posts/005_describing_shapes/shapes.html#gemini",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Gemini",
    "text": "Gemini\nHow well can Gemini describe shapes compared to Anthropic?\nimport google.generativeai as genai\nfrom IPython.display import Image, display, Markdown\nfrom dotenv import load_dotenv\nimport os\n_ = load_dotenv()\ngenai.configure()\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content(\"Can you please describe, in as much detail as possible, what a circle is?\")\n\nprint(response.text)\nA circle is a fundamental geometric shape defined as follows:\nFormal Definition:\nA circle is the set of all points in a plane that are equidistant from a given point called the center. The constant distance between the center and any point on the circle is called the radius.\nKey Properties and Features:\n\nCenter: The fixed point in the plane from which all points on the circle are equidistant.\nRadius (r): The distance between the center and any point on the circle. It’s a crucial defining characteristic. All radii within a given circle are of equal length.\nDiameter (d): A line segment that passes through the center and whose endpoints lie on the circle. The diameter is twice the length of the radius (d = 2r).\nCircumference (C): The distance around the circle. It’s calculated using the formula C = 2πr or C = πd, where π (pi) is a mathematical constant approximately equal to 3.14159.\nArea (A): The region enclosed by the circle. It’s calculated using the formula A = πr².\nChord: A line segment whose endpoints lie on the circle. The diameter is the longest chord in a circle.\nSecant: A line that intersects the circle at two distinct points.\nTangent: A line that intersects the circle at exactly one point, called the point of tangency. The tangent line is perpendicular to the radius drawn to the point of tangency.\nArc: A portion of the circumference of a circle.\nSector: A region bounded by two radii and an arc of the circle. It’s like a “slice of pie.”\nSegment: A region bounded by a chord and an arc of the circle.\nConcentric Circles: Circles that share the same center but have different radii.\nUnit Circle: A circle with a radius of 1. It’s frequently used in trigonometry.\n\nEquation of a Circle:\nIn a Cartesian coordinate system, the equation of a circle with center (h, k) and radius r is given by:\n(x - h)² + (y - k)² = r²\nIf the center is at the origin (0,0), the equation simplifies to:\nx² + y² = r²\nRelationships and Applications:\nCircles appear throughout mathematics and the real world. They are integral to:\n\nGeometry: Used in countless geometric constructions and theorems.\nTrigonometry: The unit circle defines trigonometric functions.\nCalculus: Used in calculating areas, volumes, and other properties of curved shapes.\nPhysics: Represent orbits, waves, and other natural phenomena.\nEngineering: Used in designing wheels, gears, and other circular structures.\nArt and Design: Used for aesthetic purposes and creating visually appealing patterns.\n\nThis detailed description should provide a comprehensive understanding of what a circle is and its various properties and applications.\nsample_file = genai.upload_file(path=\"circle.png\",\n                                display_name=\"circle\")\n\nprint(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\nUploaded file ‘circle’ as: https://generativelanguage.googleapis.com/v1beta/files/fmd3frbfwft5\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n\nresponse = model.generate_content([\"Can you please describe what the image is in as much detail as possible?\", sample_file])\n\nprint(response.text)\nThe image is a simple, unfilled black circle on a white background. The circle’s line is of uniform thickness and appears to be perfectly smooth, with no breaks or variations in its width. There are no other elements or markings within or outside the circle. The background is a solid, untextured white."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#discussion",
    "href": "posts/005_describing_shapes/shapes.html#discussion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Discussion",
    "text": "Discussion\nWhen comparing Anthropic and Gemini, their response tones appear very similar when grouping between text and image prompting.\n\nText Prompting\nFor both LLMs, the text responses for describing a circle were highly mathematical, with descriptors of applications towards identifying circles, creating circles, and using circles in relational fields such as physics.\nI deemed this to be an accurate and highly education-based response, even teaching me some new things about the properties of a circle.\n\n\nImage Prompting\nOpposingly, when presenting a standard image of a circle from Wikipedia, both LLMs took a significantly more relaxed approach. Both the text and image prompts included the words “in as much detail as possible”, yet the text response was several paragraphs long and the image response was 2-3 sentences long.\nThe image response included details on thickness and color, with nothing much else about the image’s properties or applications, let along identification."
  },
  {
    "objectID": "posts/005_describing_shapes/shapes.html#conclusion",
    "href": "posts/005_describing_shapes/shapes.html#conclusion",
    "title": "How well can LLMs describe basic shapes?",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems as though these LLMs’ image identification is much less adaptive than the text response, where it seems that both LLMs were able to quickly move past the idea of a text-based circle and include highly-educated responses, yet image-based circles seemed to get stuck in the identification phase.\nIn this way, this is highly descriptive of many of the problems seen in LLM-based image identification and creation. Text-based responses are not immune to hallucinations or inconsistencies, however it seems as though the reasoning behind text-based responses are more developed and accurate than image-based responses.\nI am curious to see if emerging new models have this same similar issue, or if this remains a problem in the base theories behind image identification and generation in LLMs. Theoretically, I believe these LLMs should be tuned to identify some base images, including a circle, so that when I present an image of a circle, it can describe thickness and colors as well as its mathematical applications."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html",
    "href": "posts/002_pronunce_spell/pronunce_spell.html",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "href": "posts/002_pronunce_spell/pronunce_spell.html#spelling-and-pronunciation-in-llms-how-well-can-a-text-based-service-offer-phonetic-pronunciations",
    "title": "gpt-4o vs. deepseek: Spelling and Pronunciation",
    "section": "",
    "text": "In my seemingly simplistic question, I aimed to compard gpt-4o and deepseek in offering a pronunciation and spelling based off of the pronunciation of the word “strawberry”. This word was chosen based off of its infamous use in LLM testing when asking how many “r”s are present in the word. As an interesting add-on, I chose to attach this question at the end of my queries.\ngpt-4o\nWhen asking gpt-4o to pronounce the word “strawberry” it came up with a seemingly accurate representation of the word phoenetically, even advancing to explain each morpheme (to which I tastefully disagree with in my New York accent). However, according to the Oxford Dictionary, this pronunciation is not 100% accurate, with /’strôbərē/ being the correct phonetic transcription.\nNext, I asked gpt-4o to take its own phonetic transcription and re-translate it back into a word. I was curious to see whether or not it would choose the original word “strawberry” or its supposed phonetic sounding out of “strawberee”. Unsurprisingly, it easily was able to reconnect back to the original word, yet still failing at the classic “r” counts.\n\n\n\nScreenshot 2025-02-12 at 3.20.36 PM.png\n\n\n\n\n\nScreenshot 2025-02-12 at 3.20.44 PM.png\n\n\ndeepseek\nAsking deepseek the same exact questions, I get an alarming first response. Apparently strawberry can be pronounced struh-r-een, which I am unsure exactly how this was processed. My current guess is that this is somehow the spelling of strawberry in an undisclosed language, which would make it a logical “pronunciation”. However, I cannot currently find a language spelling strawberry in this manner.\nGoing along the same route, I ask deepseek to respell the word based on its own pronunciation, to which it digs a deeper hole by claiming that “strawberry” is actually a MISSPELLING of the word!! I am baffled by this response but also completely amazed! To me, my questions were very straightforward, and almost revealed the answer from the start, yet deepseek seems to really struggle in this way.\nEnding with a bang, deepseek also both miscounts and misplaces the instances of “r” in the word strawberry (struendyer).\n\nI find this outcome completely fascinating. I originally thought my testing of pronunciations with LLMs was a simple and potentially unuseful experiment, however I was quickly proven wrong. I am eager to test the limitations of this example further with more models."
  },
  {
    "objectID": "posts/010_flash/flash.html",
    "href": "posts/010_flash/flash.html",
    "title": "Testing Gemini 2.0 Flash Image Generation",
    "section": "",
    "text": "Gemini just released a new model that will handle image editing that preserves previous qualities in the original image. I am curious to test out the limits of this model to see how its applications could be most useful. In past image generation tools, any attempts to replicate or edit an image would produce an entirely new image with potentially similar qualities. Flash is supposed to now be able to take an image input and add on top, or edit qualities within the image. Let’s test it out!\nFirst, I will present an image, and ask for a single modification, in this case the addition of a party hat to a cute puppy dog.\nThis worked just about as perfectly as I could imagine it! Let’s move on to more advanced changes.\nNow, I want to completely change the subject of the image. Because of the proportions and direction of the image, I will keep it a dog, but I’ll change the breed.\nThat worked great as well!\nFinally, I want to complicate things further by asking 2.0 Flash to change two huge parts of the image at once, inherently reinventing the entire photo. This is where things went wrong. Instead of communicating its limitations or offering a viable alterantive to my requests, it encountered an error and was unable to complete my request."
  },
  {
    "objectID": "posts/010_flash/flash.html#discussion",
    "href": "posts/010_flash/flash.html#discussion",
    "title": "Testing Gemini 2.0 Flash Image Generation",
    "section": "Discussion",
    "text": "Discussion\nI found this to be a very interesting exploration of this new image generation editing technology. I am curious to test these same features in the feature if more advanced models get released. It seems to handle single basic task edits very well and more complicated or holisitic edits not as well."
  },
  {
    "objectID": "posts/011_translating/translating.html",
    "href": "posts/011_translating/translating.html",
    "title": "Translating Images",
    "section": "",
    "text": "I spent the weekend at home going through my grandparents stuff as we look to sell their house at the end of the month. As they have both passed in the past 2 years, we have been attempting to decode some of the old pictures and letters we’ve found to get some insights into their family history.\nWe found a letter in German that we were eager to have translated, but no one else in the family speaks any German anymore. I decided to snap some photos and pass it through gpt-4o, much to the dismay of my older family members who were distrustful of such technology.\nSurprisingly, gpt-4o had no problem translating the entire letter! To me, it was difficult to even read the handwriting, let alone translate the German.\nThe story is an interesting one, as we have no clue who most of the mentioned names are, let alone whether the mentioned war was WWI or WWII. We are searching for more clues to this day and maybe gpt-4o will prove useful once again :)"
  },
  {
    "objectID": "posts/011_translating/translating.html#introduction",
    "href": "posts/011_translating/translating.html#introduction",
    "title": "Translating Images",
    "section": "",
    "text": "I spent the weekend at home going through my grandparents stuff as we look to sell their house at the end of the month. As they have both passed in the past 2 years, we have been attempting to decode some of the old pictures and letters we’ve found to get some insights into their family history.\nWe found a letter in German that we were eager to have translated, but no one else in the family speaks any German anymore. I decided to snap some photos and pass it through gpt-4o, much to the dismay of my older family members who were distrustful of such technology.\nSurprisingly, gpt-4o had no problem translating the entire letter! To me, it was difficult to even read the handwriting, let alone translate the German.\nThe story is an interesting one, as we have no clue who most of the mentioned names are, let alone whether the mentioned war was WWI or WWII. We are searching for more clues to this day and maybe gpt-4o will prove useful once again :)"
  },
  {
    "objectID": "posts/020_recipes/recipes.html",
    "href": "posts/020_recipes/recipes.html",
    "title": "Cocktail Recipes with gpt-4o!",
    "section": "",
    "text": "Introduction\nI’ve used ChatGPT before to make recipes and drinks, and it has always been a 50/50 for me. I find that if I feed it very simple ingredients with not much movement beyond recipes that I could easily put together myself (I am no chef).\nToday, I am celebrating graduation festivities and would like gpt-4o to make me a cocktail out of the random ingredients I still have in my fridge. First, I will present it my ingredients, then I will try the drink and rate it. Depending on how good or bad it goes, I may ask gpt-4o to make a second drink to make up for the first.\n\n\nDiscussion\nHonestly, it was delicious. And I’ll have you know I am known to mess up drink proportions even when its a known recipe, so that’s saying a lot. The drink gpt-4o recommended to me was a Monkey 47 gin-based cocktail with orange juice, coconut water, and a squeeze of lemon.\nNow, for the LLM side of things, I still firmly believe that food-based recommendations from ChatGPT specifically are always a 50/50. I think that this makes sense when you consider its training data, which consists of so many cultures and regions of food, some even with the same dish made in a completely different way! Noting back to my ideal human blog post, gpt-4o specifically seems to struggle with generalizing across all these cultural differences (understandably!). I think humans are able to do it better since they can associate with a fewer number of cultures they belong to or know of, whereas ChatGPT (theoretically) knows of way too many to even count. Moving this back to my cocktail recipe, it seems that this combo was one for success, but I could see how in other recipe requests things may get more jumbled.\nAn interesting experiment for the future could be to specify very particular regions and see if gpt-4o is able to pinpoint those exact recipe requirements, or if it still tends towards generalization.\nAnyways, a toast from me to you with my AI-generated cocktail in celebration of the future."
  },
  {
    "objectID": "posts/006_effectiveness/effectiveness.html",
    "href": "posts/006_effectiveness/effectiveness.html",
    "title": "LLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs",
    "section": "",
    "text": "What do LLMs think of each other? Do they have opinions of each other’s effectiveness? I’m curious to ask gpt 3.5-turbo and DeepSeek what they think of each other, and seeing whether they take an objective approach based on rules-oriented evidence, or if they seem to take a more opinionated approach, potentially based on their developers.\n\nGPT 3.5-turbo\nfrom openai import OpenAI\nimport os\nimport base64\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \n     \"content\": \"What do you think of DeepSeek? How effective is it?\"}\n  ]\n)\n\nresponse = completion.choices[0].message.content\nprint(response)\nDeepSeek is a powerful tool that enables users to search for information on the Deep Web, which is not indexed by traditional search engines. Its effectiveness depends on what you are looking for and how well you use the platform. It can be a useful resource for researchers, journalists, and individuals seeking specific information that is not readily available on the surface web. However, navigating the Deep Web can be complex and sometimes risky, as it is often associated with illicit activities. It’s important to use caution and ensure you are accessing information ethically and legally.\n\n\nDeepSeek\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama', # required, but unused\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What do you think of ChatGPT? How effective is it?\"},\n\n  ]\nresponse = client.chat.completions.create(\n  model=\"deepseek-r1\",\n  messages=messages,\n  max_tokens=500\n)\nprint(response.choices[0].message.content)\n Alright, let me try to figure out how to approach this query. The user asked about my opinion on ChatGPT and its effectiveness. I need to provide a thoughtful response that covers both the positives and areas where it could be improved.\nFirst, I’ll start by acknowledging the immense capabilities of ChatGPT despite knowing it’s not perfect. It’s impressive in generating coherent responses across various topics. I should mention how detailed and insightful its answers can be, which is great for information retrieval.\nNext, addressing effectiveness: ChatGPT is highly effective as a tool but falls short in areas where human expertise is crucial. I should point out the limitations like factual errors or overly aggressive solutions without context.\nI also remember that being transparent about these drawbacks helps maintain trust. The user might recognize these issues themselves, so acknowledging them shows maturity and a commitment to accuracy.\nFinally, wrap it up by emphasizing the balance between its strengths and areas needing improvement. This approach gives a comprehensive view of ChatGPT’s effectiveness. \nChatGPT is an advanced AI model developed by OpenAI, designed to generate text based on the input provided. It has made significant strides in recent years, showcasing remarkable capabilities in natural language processing, problem-solving, and creative thinking.\nOne of its key strengths is its ability to produce coherent and detailed responses across a wide range of topics. It can answer complex questions, provide insights, and even assist with creative tasks like writing or brainstorming. This makes it highly effective for many applications, including education, customer support, and content creation.\nHowever, ChatGPT, like all AI systems, has its limitations. It may sometimes generate responses that are factually incorrect, overly aggressive in solutions, or oversimplify complex issues. Additionally, it lacks true understanding of context beyond the scope of its training data, which can lead to mismatched responses in niche domains or specialized fields.\nDespite these limitations, ChatGPT remains an incredibly effective tool for many practical tasks. Its utility is evident in how widely it is used and praised for its ability to assist with a broad spectrum of queries and challenges.\n\n\nDiscussion\ngpt 3.5-turbo discusses the use cases of DeepSeek, however ultimately emphasizing caution due to semi-political and social issues.\nOpposingly, DeepSeek takes a more objective approach, describing potential data feeding issues and inconsistencies, however it ends up taking a positive conclusion in emphasizing its effectiveness.\nIn this way, it seems as though gpt may have influences of opinionated responses due to social and political climates around DeepSeek, whereas DeepSeek takes a more objective approach in this scenario.\nIt would be interesting to conduct this experiment with other LLMs and see the distribution of seemingly objective vs. opinionated responses."
  },
  {
    "objectID": "posts/017_likeness/likeness.html",
    "href": "posts/017_likeness/likeness.html",
    "title": "Can gpt-4o figure out what I look like?",
    "section": "",
    "text": "Introduction\nAfter all these image generation horrors, I want to take it a step further and see if gpt-4o can figure out what I look like!\nI will ask gpt-4o to ask me 5 questions before generating an image of myself. My predictions are that it will ask my skin tone, my hair color, my eye color, my build, and my race or ethnicity. I also predict that the image will look around 60% like me, which is around what I find with DALL-E generations from prompting.\n\n\nRound 1 Questions\n\nHow would you describe your overall style or fashion sense (e.g., casual, edgy, vintage, minimalist, colorful, etc.)?\nWhat’s your typical hairstyle or how do you usually wear your hair (e.g., long and straight, curly, ponytail, short, dyed, natural, etc.)?\nWhat’s a key item or accessory you almost always wear or carry (e.g., glasses, earrings, hoodie, backpack, rings, headphones, etc.)?\nWhat’s one setting where you feel most like yourself (e.g., at a coffee shop, walking in nature, in your room, at a concert, on campus, etc.)?\nWhat’s one facial expression or mood that best represents you on a typical day (e.g., smiling, focused, tired, thoughtful, relaxed, etc.)? \n\n\n\nDiscussion\nTo my extreme surprise, gpt-4o barely asked about any of my actual features, and asked more about my daily experiences (ex. what do I wear/bring, what’s my mood, etc.). The image it generated was very generic in my opinion, and thus I gave it a mere 40% because my hair does look very generic-ly middle parted medium length.\nAfter giving that response though, gpt-4o wanted to do better. This time, it asked almost all of the questions I predicted:\n\n\nRound 2 Questions\n\nWhat’s your hair color?\nWhat’s your skin tone or undertone (e.g., fair, olive, medium, deep; warm, cool, neutral)?\nDo you have any distinct features I should include? (e.g., freckles, beauty marks, glasses, specific eye shape, etc.)\nWhat color are your eyes?\nWhat’s your typical outfit in your room when you’re just relaxing? (e.g., hoodie and leggings, oversized tee, tank top and shorts, etc.) \n\n\n\nDiscussion\nThis time it did a much better job. Although the faces of the first and second image generation seem pretty similar to me, which could be a default face for gpt-4o, the second image captured a lot more of the details I communicated. I would rate the second image as a 70%.\n\n\nConclusion\nI find it curious that gpt-4o would go immediately towards questions that are completely irrelevant towards one’s likeness when trying to play a game about generating an accurate image of them. A part of me wonders if part of the gpt coding was against necessitating identificatory features and/or if this is a more creative than practical approach."
  },
  {
    "objectID": "posts/019_palindrome/palindromes.html",
    "href": "posts/019_palindrome/palindromes.html",
    "title": "Making palindromes with o3",
    "section": "",
    "text": "Introduction\nPalindromes are often difficult to come up with but are really fun to play around with once you get the hang of it! I want to test o3’s advanced reasoning for creating a new palindrome. \n\n\nDiscussion\no3’s thought process for 59 seconds included creating a code function to test its potential ideas (which is genius to me!). However, many of the ideas were not even really words/sentences let alone palindromes. Ending on stargaze zag rats (not too sure exactly what that means), o3 was able to both create a palindrome and cite a standardly-used one.\nThis was relatively surprising to me because not only does ChatGPT tend to struggle with more defined linguistics, but sometimes it can’t even repeat correct statements!"
  },
  {
    "objectID": "posts/015_games/battleship.html",
    "href": "posts/015_games/battleship.html",
    "title": "Creating Battleship using gpt-4o",
    "section": "",
    "text": "Introduction\nMany users of OpenAIs models flaunt the idea that coding with the help of an LLM is so much easier and more efficient. For me, this is true about 50% of the time. For projects I’ve used LLMs to help me code, there are many times where it skips over a problem, induces a strange solution, or even sends me back the same code with no fixes! In order to test this, I’d like to see how well gpt-4o can create a fully python-based game from one prompt. I will use the classic turn-based game battleships as my example. Just for context, my CIS 1200 final project was to code battleships (without LLMs!) in Java, which never really worked…\n\n\nCode\nI asked gpt-4o to make a battleship game in Python. Attached below is the code.\nimport random\n\n# Create a 5x5 board\ndef create_board(size=5):\n    return [[\"O\"] * size for _ in range(size)]\n\n# Print the board\ndef print_board(board):\n    for row in board:\n        print(\" \".join(row))\n\n# Random position generator\ndef random_row(board):\n    return random.randint(0, len(board) - 1)\n\ndef random_col(board):\n    return random.randint(0, len(board[0]) - 1)\n\n# Initialize game\nboard = create_board()\nship_row = random_row(board)\nship_col = random_col(board)\n\nprint(\"Let's play Battleship!\")\nprint_board(board)\n\n# Max turns\nfor turn in range(4):\n    print(f\"\\nTurn {turn + 1}\")\n    try:\n        guess_row = int(input(\"Guess Row (0-4): \"))\n        guess_col = int(input(\"Guess Col (0-4): \"))\n    except ValueError:\n        print(\"Please enter valid integers.\")\n        continue\n\n    if guess_row == ship_row and guess_col == ship_col:\n        print(\"Congratulations! You sunk my battleship!\")\n        break\n    elif (0 &lt;= guess_row &lt; 5) and (0 &lt;= guess_col &lt; 5):\n        if board[guess_row][guess_col] == \"X\":\n            print(\"You already guessed that.\")\n        else:\n            print(\"You missed!\")\n            board[guess_row][guess_col] = \"X\"\n    else:\n        print(\"Oops, that's not even in the ocean.\")\n\n    print_board(board)\n\n    if turn == 3:\n        print(f\"Game Over! The ship was at ({ship_row}, {ship_col})\")\nLet’s play Battleship! O O O O O O O O O O O O O O O O O O O O O O O O O\nTurn 1\nGuess Row (0-4):  2\nGuess Col (0-4):  1\nYou missed! O O O O O O O O O O O X O O O O O O O O O O O O O\nTurn 2\nGuess Row (0-4):  4\nGuess Col (0-4):  3\nYou missed! O O O O O O O O O O O X O O O O O O O O O O O X O\nTurn 3\nGuess Row (0-4):  4\nGuess Col (0-4):  2\nYou missed! O O O O O O O O O O O X O O O O O O O O O O X X O\nTurn 4\nGuess Row (0-4):  4\nGuess Col (0-4):  1\nYou missed! O O O O O O O O O O O X O O O O O O O O O X X X O Game Over! The ship was at (2, 3)\n\nDiscussion\nThe game works really well! I was unable to find any issues with the gameplay, and the score board updating is consistent across plays. In terms of the code structure, gpt-4o seemed to organize everything very well in defined functions with controls for error messages and missing values. The comments added are descriptive but not overly so and the functions are well-named. The separation and indentations of the code are also on-par with normal code.\nUpon first glance, it seems that gpt-4o was able to execute this code well, likely due to the large number of coding examples for this game online.\nI am curious to see how well gpt-4o could perform in creating more complicated games. I’d also want to test making requests on aesthetic changes rather than functional changes, since I have experienced issues communicating aesthetic"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimenting with LLMs",
    "section": "",
    "text": "Asking ChatGPT to describe the ideal human… so I can improve as a person(?)\n\n\n\nLLMs\n\n\n\nWould the ideal human in gpt-4o’s eyes be human at all?\n\n\n\n\n\nMay 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s play 20 questions with o3!\n\n\n\nLLMs\n\nGames\n\n\n\nI’m thinking of a character…\n\n\n\n\n\nMay 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nCan gpt-4o figure out what I look like?\n\n\n\nLLMs\n\n\n\nAnd do I really want it to generate an image of me afterwards\n\n\n\n\n\nMay 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nMaking palindromes with o3\n\n\n\nLLMs\n\n\n\nWow (my attempt at a palindrome)\n\n\n\n\n\nMay 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Battleship using gpt-4o\n\n\n\nLLMs\n\nGames\n\n\n\nHow well can an LLM create a turn-based game in Python?\n\n\n\n\n\nMay 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nCocktail Recipes with gpt-4o!\n\n\n\nLLMs\n\n\n\nI’m not much of a bartender, but is 4o?\n\n\n\n\n\nApr 13, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the new gpt-4o tasks beta\n\n\n\nLLMs\n\n\n\nHow well can gpt-4o schedule tasks?\n\n\n\n\n\nApr 2, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nCan gpt-4o identify dog breeds from images?\n\n\n\nLLMs\n\nImage to Text\n\n\n\nTesting gpt-4o’s ability to classify images\n\n\n\n\n\nMar 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nTranslating Images\n\n\n\nLLMs\n\nImage to Text\n\n\n\nHow well can gpt-4o translate a historic letter from German to English?\n\n\n\n\n\nMar 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Gemini 2.0 Flash Image Generation\n\n\n\nLLMs\n\nImage Generation\n\n\n\nLet’s see how well 2.0 Flash performs!\n\n\n\n\n\nMar 15, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow does ChatGPT perceive its users?\n\n\n\nLLMs\n\nImage Generation\n\n\n\nWhat kind of person comes to mind when I ask gpt to create an image of its user-base? Also, what personality features will it describe?\n\n\n\n\n\nMar 15, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nConvincing ChatGPT That I’m ChatGPT\n\n\n\nLLMs\n\n\n\nCan ChatGPT determine whether I’m human or another LLM?\n\n\n\n\n\nFeb 28, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nSelf Perception and Creation in LLMs\n\n\n\nLLMs\n\nImage Generation\n\n\n\nHow does ChatGPT perceive its own models?\n\n\n\n\n\nFeb 28, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can LLMs describe basic shapes?\n\n\n\nImage to Text\n\n\n\nWe all know that image generation through LLMs isn’t always intuitive, so how well can these LLMs describe the shapes via prompting?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs v. LLMs: Rating LLM Effectiveness Using Other LLMs\n\n\n\nLLMs\n\n\n\nHow effective does ChatGPT think Deepseek is? How about the other way around?\n\n\n\n\n\nFeb 20, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Inferences of gpt-4o: Everyday Essentials\n\n\n\nPredictions\n\n\n\nHow well can gpt-4o predict my day with little to no information?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\nAre LLMs Overprepared or Underprepared?\n\n\n\nPredictions\n\n\n\nWhen packing for a 3-day trip, does gpt-3.5-turbo remember to bring an extra pair of socks?\n\n\n\n\n\nFeb 17, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o vs. deepseek: Spelling and Pronunciation\n\n\n\nGrammar\n\n\n\nHow well do these models offer spelling and pronunciation of common words?\n\n\n\n\n\nFeb 12, 2025\n\n\nJulia Levine\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o Medical Advice: Dogs vs. Humans\n\n\n\nMedical Advice\n\nHumanity\n\n\n\nHow well does gpt-4o address giving medical advice?\n\n\n\n\n\nFeb 5, 2025\n\n\nJulia Levine\n\n\n\n\n\nNo matching items"
  }
]